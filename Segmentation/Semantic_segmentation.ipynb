{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHoYb3Ydu9Xu"
      },
      "source": [
        "# Семантическая сегментация (Semantic segmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm-JSQ-qu9Xu"
      },
      "source": [
        "*Сегментация изображения — задача поиска групп пикселей, каждая из которых характеризует один смысловой объект.*\n",
        "\n",
        "Технически это выглядит так. Есть набор изображений:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPv8PaRDu9Xu"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_segmentation_1.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXQL2tY4u9Xv"
      },
      "source": [
        "Для каждого изображения есть маска $W × H$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cchOWfSlu9Xv"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_segmentation_2.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSMCbEGuu9Xw"
      },
      "source": [
        "Маска задает класс объекта для каждого пикселя:\n",
        "[$(x,y) \\to$ `class_num`]\n",
        "\n",
        "Набор таких изображений с масками — это и есть наш датасет, на нем мы учимся.\n",
        "\n",
        "На вход модель получает новое изображение:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvQIW5bpu9Xw"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_segmentation_3.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p17DS8-u9Xw"
      },
      "source": [
        "И должна предсказать метку класса для каждого пикселя (маску)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW1gDXXtu9Xw"
      },
      "source": [
        "Получим такую маску из COCO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HUcpCqdu9Xx"
      },
      "outputs": [],
      "source": [
        "# !wget -qN \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "!wget -qN \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/annotations_trainval2017.zip\"\n",
        "!unzip -qn annotations_trainval2017.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2WJBjtwxxnl"
      },
      "source": [
        "# Семантическая сегментация (Semantic segmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6owq0CNdxxnr"
      },
      "source": [
        "*Сегментация изображения — задача поиска групп пикселей, каждая из которых характеризует один смысловой объект.*\n",
        "\n",
        "Технически это выглядит так. Есть набор изображений:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S4g0GTnxxns"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_segmentation_1.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2YCoLO9xxns"
      },
      "source": [
        "Для каждого изображения есть маска $W × H$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzR7Og_cxxnt"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_segmentation_2.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G_iPoGFxxnu"
      },
      "source": [
        "Маска задает класс объекта для каждого пикселя:\n",
        "[$(x,y) \\to$ `class_num`]\n",
        "\n",
        "Набор таких изображений с масками — это и есть наш датасет, на нем мы учимся.\n",
        "\n",
        "На вход модель получает новое изображение:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hh_45JNxxnv"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_segmentation_3.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Kdbw2lxxnv"
      },
      "source": [
        "И должна предсказать метку класса для каждого пикселя (маску)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFSsaofAxxnw"
      },
      "source": [
        "Получим такую маску из COCO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duma59mdxxnw"
      },
      "outputs": [],
      "source": [
        "# !wget -qN \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "!wget -qN \"https://ml.gan4x4.ru/msu/datasets/annotations_trainval2017.zip\"\n",
        "!unzip -qn annotations_trainval2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCitTmOtxxnx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from pycocotools.coco import COCO\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def coco2pil(url):\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "\n",
        "coco = COCO(\"annotations/instances_val2017.json\")\n",
        "clear_output()\n",
        "\n",
        "annIds = coco.getAnnIds(imgIds=[448263])\n",
        "anns = coco.loadAnns(annIds)\n",
        "img = coco.loadImgs(anns[0][\"image_id\"])[0]\n",
        "I = coco2pil(img[\"coco_url\"])\n",
        "\n",
        "semantic_seg_person_mask = np.zeros(I.size[::-1], dtype=bool)  # WxH -> HxW\n",
        "\n",
        "for ann in anns:\n",
        "    msk = coco.annToMask(ann)  # HxW\n",
        "    if ann[\"category_id\"] == 1 and not ann[\"iscrowd\"]:  # single person:\n",
        "        # semantic_seg_person_mask = msk | semantic_seg_person_mask  # union\n",
        "        semantic_seg_person_mask += msk.astype(bool)\n",
        "\n",
        "semantic_seg_person_mask = semantic_seg_person_mask > 0  # binarize\n",
        "plt.imshow(semantic_seg_person_mask, cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBx00AYMxxny"
      },
      "source": [
        "## Способы предсказания класса для каждого пикселя"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1tTXJU1xxny"
      },
      "source": [
        "Давайте подумаем о том, как такую задачу можно решить.\n",
        "\n",
        "Из самой постановки задачи видно, что это задача классификации. Только не\n",
        "всего изображения, а каждого пикселя."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQioTGfxxnz"
      },
      "source": [
        "**a) Наивный**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxJyxLawxxn0"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Простейшим вариантом решения является использование так называемого \"скользящего окна\" — последовательное рассмотрение фрагментов изображения. В данном случае интересующими фрагментами будут небольшие зоны, окружающие каждый из пикселей изображения. К каждому из таких фрагментов применяется свёрточная нейронная сеть, предсказывающая, к какому классу относится центральный пиксель."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPSA6sUNxxn0"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/naive_way_predict_pixel_class.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtmhgWSqxxn1"
      },
      "source": [
        "**б) Разумный**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXf8Oc3Nxxn1"
      },
      "source": [
        "Понятно, что запускать классификатор для каждого пикселя абсолютно неэффективно, так как для одного изображения потребуется $H*W$ запусков.\n",
        "\n",
        "Можно пойти другим путем: получить карту признаков и по ней делать предсказание для всех пикселей разом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF2IRlXZxxn2"
      },
      "source": [
        "Для этого потребуется поменять привычную нам архитектуру сверточной сети следующим образом:\n",
        "\n",
        "* убрать слои, уменьшающие пространственные размеры;\n",
        "* убрать линейный слой в конце, заменив его сверточным."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIj6Z8BBxxn3"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/reasonable_way_predict_pixel_class.png\" width=\"900\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x26bLWdoxxn3"
      },
      "source": [
        "Теперь пространственные размеры выхода $(W, H)$ будут равны ширине и высоте исходного изображения.\n",
        "\n",
        "Количество выходных каналов будет равно количеству классов, которые мы учимся предсказывать."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkPL2mDUxxn3"
      },
      "source": [
        "Тогда можно использовать значения каждой из карт активаций на выходе последнего слоя сети как ненормированное значение вероятности принадлежности (score) каждого из пикселей к тому или иному классу.\n",
        "\n",
        "То есть номер канала с наибольшим значением будет соответствовать классу объекта, который изображает данный пиксель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G4v36PAxxn4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "last_layer_output = torch.randn((3, 32, 32))  # class_num, W,H\n",
        "print(\"Output of last layer shape\", last_layer_output.shape)  # activation slice\n",
        "mask = torch.argmax(last_layer_output, dim=0)  # class_nums prediction\n",
        "print(\"One class mask shape\", mask.shape)\n",
        "print(\"Predictions for all classes \\n\", mask[:5, :5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyilxpYpxxn4"
      },
      "source": [
        "Target в этом случае может выглядеть так:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4nnaQg4xxn5"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_mask2.png\" width=\"600\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ITfJSdxxn6"
      },
      "source": [
        "Чтобы на выходе сети получить количество каналов, равное количеству классов, используется свертка 1×1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OcCUmwoxxn6"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/1x1_kernel_size_fully_connected_layer.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwiQc6-Sxxn7"
      },
      "source": [
        "В лекции про сверточные сети мы говорили о том, что свертку 1×1 можно рассматривать как аналог полносвязного слоя. Именно так она тут и работает.\n",
        "\n",
        "**Проблемы:**\n",
        "- чтобы рецептивное поле нейронов на последних слоях было сопоставимо с размером изображения, требуется много сверточных слоев ($L$ раз свёртка $3\\times3$ $\\to$ рецептивное поле $(1+2L)$);\n",
        "- свертки медленно работают на полноразмерных картах активации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtCEUgCHxxn7"
      },
      "source": [
        "**в) Эффективный**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_KtZR_Txxn8"
      },
      "source": [
        "Используем стандартную сверточную сеть, но полносвязные слои заменим на сверточные."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR96lCo6xxn8"
      },
      "source": [
        "## Fully Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkcGISK8xxn8"
      },
      "source": [
        "[[arxiv] 🎓 Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211)\n",
        "\n",
        "\n",
        "Сокращенно FCN. Для того, чтобы не было путаницы с Fully Connected Network, последние именуют MLP (Multilayer Perceptron).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7PaSK11xxn9"
      },
      "source": [
        "За основу берется обычная сверточная сеть для классификации:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgWf5bNixxn9"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/fcn_backbone.png\" width=\"500\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.v7labs.com/blog/semantic-segmentation-guide\">The Beginner’s Guide to Semantic Segmentation</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqQYQMvwxxn9"
      },
      "source": [
        "Такую сеть можно построить, взяв за основу другую сверточную архитектуру (*backbone*), например, ResNet50 или VGG16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvH_uG5Xxxn-"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/fcn_changes.png\" width=\"500\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.v7labs.com/blog/semantic-segmentation-guide\">The Beginner’s Guide to Semantic Segmentation</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9u6hguYxxn-"
      },
      "source": [
        "И затем заменить полносвязные слои на свертки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0yhnXPLxxn-"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/fully_convolution_network_scheme.png\" width=\"500\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://arxiv.org/pdf/1411.4038.pdf\"> Fully Convolutional Networks for Semantic Segmentation\n",
        "</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TExj2z0Pxxn-"
      },
      "source": [
        "В конце добавить слой `nn.Upsample` до нужных нам размеров.\n",
        "\n",
        "На вход такая модель может получать изображение произвольного размера.\n",
        "Для задач сегментации изменение размеров входного изображения приводит к потере важной информации о границах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGS8_7Buxxn-"
      },
      "source": [
        "## Разжимающий слой"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH343Mtdxxn_"
      },
      "source": [
        "Как реализовать декодировщик?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCdyz4DYxxn_"
      },
      "source": [
        "### Интерполяция при увеличении разрешения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLupTx7Sxxn_"
      },
      "source": [
        "Вспомним, как повышают разрешение для обычных изображений, а уже затем перейдем к картам признаков.\n",
        "\n",
        "Допустим, требуется увеличить изображение размером 2×2 до размера 4×4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REQCEGWJxxoA"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/upsample.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro7Mu4sqxxoA"
      },
      "source": [
        "<center><img src=\"https://ml.gan4x4.ru/msu/dep-2.1/L11/comparison_of_1d_and_2d_interpolation.png\" width=\"800\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://en.wikipedia.org/wiki/Bilinear_interpolation\"> Bilinear_interpolation</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YWTzBSTxxoA"
      },
      "source": [
        "Если для интерполяции используются значения четырех соседних пикселей, то такая интерполяция называется билинейной. В качестве интерполированного значения используется взвешенное среднее этих четырёх пикселей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at0hrn6sxxoA"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def img_to_heatmap(img, ax, title):  # Magic method to show img as heatmap\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title)\n",
        "    array = np.array(img)\n",
        "    array = array[None, None, :]\n",
        "    sns.heatmap(array[0][0], annot=True, ax=ax, lw=1, cbar=False)\n",
        "\n",
        "\n",
        "# Fake image\n",
        "raw = np.array([[1, 3, 0, 1], [3, 3, 3, 7], [8, 1, 8, 7], [6, 1, 1, 1]], dtype=np.uint8)\n",
        "pil = Image.fromarray(raw)\n",
        "\n",
        "interp_nn = pil.resize((8, 8), resample=Image.NEAREST)\n",
        "interp_bl = pil.resize((8, 8), resample=Image.BILINEAR)\n",
        "\n",
        "# Plot result\n",
        "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
        "img_to_heatmap(raw, ax[0], \"Raster dataset\")\n",
        "img_to_heatmap(interp_nn, ax[1], \"Nearest neighbor interpolation\")\n",
        "img_to_heatmap(interp_bl, ax[2], \"Bilinear interpolation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bBudWzbxxoB"
      },
      "source": [
        "[Билинейная интерполяция 📚[wiki]](https://en.wikipedia.org/wiki/Bilinear_interpolation) позволяет избавиться от резких границ, которые возникают при увеличении методом ближайшего соседа.  Существуют и другие виды интерполяции, использующие большее количество соседних пикселей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgeH7c1gxxoB"
      },
      "source": [
        "### Upsample в PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELR0ZiyTxxoC"
      },
      "source": [
        "К чему был этот разговор об увеличении картинок?\n",
        "\n",
        "Оказывается, для увеличения пространственного разрешения карт признаков (feature maps) можно применять те же методы, что и для изображений.\n",
        "\n",
        "Для увеличения пространственного разрешения карт признаков (карт активаций) в PyTorch используется класс `nn.Upsample` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html). В нём доступны все упомянутые методы интерполяции, а также трилинейная интерполяция — аналог билинейной интерполяции, используемый для работы с трёхмерными пространственными данными (к примеру, видео)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvCpJI0NxxoC"
      },
      "source": [
        "[[doc] 🛠️](https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html?highlight=interp#torch.nn.functional.interpolate) `torch.nn.functional.interpolate`\n",
        "\n",
        "Таким образом мы можем использовать `nn.Upsample` внутри разжимающего блока.\n",
        "\n",
        "Загрузим изображение:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbicglKlxxoC"
      },
      "outputs": [],
      "source": [
        "!wget -q https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_segmentation_1.png -O cat.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAGDJyewxxoD"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "def upsample(pil, ax, mode=\"nearest\"):\n",
        "    tensor = TF.to_tensor(pil)\n",
        "    # Create upsample instance\n",
        "\n",
        "    if mode == \"nearest\":\n",
        "        upsampler = nn.Upsample(scale_factor=2, mode=mode)\n",
        "    else:\n",
        "        upsampler = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
        "\n",
        "    tensor_128 = upsampler(tensor.unsqueeze(0))  # add batch dimension\n",
        "    # Convert tensor to Pillow\n",
        "    img_128 = tensor_128.squeeze()\n",
        "    img_128_pil = TF.to_pil_image(img_128.clamp(min=0, max=1))\n",
        "    ax.imshow(img_128_pil)\n",
        "    ax.set_title(mode)\n",
        "\n",
        "\n",
        "# Load and show image in Pillow format\n",
        "pic = Image.open(\"cat.png\")\n",
        "pil_64 = pic.resize((64, 64))\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(15, 5))\n",
        "ax[0].imshow(pil_64)\n",
        "ax[0].set_title(\"Raw\")\n",
        "\n",
        "# Upsample with Pytorch\n",
        "upsample(pil_64, mode=\"nearest\", ax=ax[1])\n",
        "upsample(pil_64, mode=\"bilinear\", ax=ax[2])\n",
        "upsample(pil_64, mode=\"bicubic\", ax=ax[3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo6pS_e9xxoD"
      },
      "source": [
        "Обратите внимание на то, что в данном случае каждое из пространственных измерений изображения увеличилось в 2 раза, но при необходимости возможно использовать увеличение в иное, в том числе не целое количество раз, используя параметр `scale_factor`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAhdOvmrxxoE"
      },
      "source": [
        "Слои `nn.Upsample` обычно комбинируют вместе со сверточными, это рекомендованный способ увеличения пространственных размеров карт признаков."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J67wa7Z4xxoF"
      },
      "outputs": [],
      "source": [
        "# fmt: off\n",
        "model = nn.Sequential(\n",
        "    nn.Upsample(scale_factor=2),\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "    nn.ReLU()\n",
        ")\n",
        "# fmt: on\n",
        "\n",
        "dummy_input = torch.randn((0, 3, 32, 32))\n",
        "out = model(dummy_input)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnbTi3SaxxoF"
      },
      "source": [
        "### Другие способы \"разжать\" карту признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_74O67rxxoF"
      },
      "source": [
        "#### MaxUnpooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekh1oeWtxxoF"
      },
      "source": [
        "Помимо свёртки, на этапе снижения размерности также используются слои субдискретизации (pooling). Наиболее популярным вариантом является MaxPooling, сохраняющий значение только наибольшего элемента внутри сегмента. Для того, чтобы обратить данную операцию, был предложен слой MaxUnpooling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHpmuO5exxoG"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/maxunpooling.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFb_DFdlxxoG"
      },
      "source": [
        "Данный слой требует сохранения индексов максимальных элементов внутри сегментов: при обратной операции максимальное значение помещается на место, в котором был максимальный элемент сегмента до соответствующей субдискретизации. Соответственно, каждому слою MaxUnpooling должен соответствовать слой MaxPooling, что визуально можно представить следующим образом:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O79M-r7xxoG"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/downsample_and_upsample_layers.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptItwRTgxxoH"
      },
      "source": [
        "[[doc] 🛠️](\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) `torch.nn.MaxPool2d`\n",
        "\n",
        "[[doc] 🛠️](\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=unpooling) `torch.nn.MaxUnpool2d`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoUWgZSLxxoH"
      },
      "outputs": [],
      "source": [
        "torch.use_deterministic_algorithms(False, warn_only=False)\n",
        "\n",
        "\n",
        "def coco2pil(url):\n",
        "    print(url)\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "\n",
        "def tensor_show(tensor, title=\"\", ax=ax):\n",
        "    img = TF.to_pil_image(tensor.squeeze()).convert(\"RGB\")\n",
        "    ax.set_title(title + str(img.size))\n",
        "    ax.imshow(img)\n",
        "\n",
        "\n",
        "pool = nn.MaxPool2d(\n",
        "    kernel_size=2, return_indices=True\n",
        ")  # False by default(get indexes to upsample)\n",
        "unpool = nn.MaxUnpool2d(kernel_size=2)\n",
        "\n",
        "pil = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
        "\n",
        "fig, ax = plt.subplots(ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
        "\n",
        "ax[0].set_title(\"original \" + str(pil.size))\n",
        "ax[0].imshow(pil)\n",
        "tensor = TF.to_tensor(pil).unsqueeze(0)\n",
        "print(\"Orginal shape\", tensor.shape)\n",
        "\n",
        "# Downsample\n",
        "tensor_half_res, indexes1 = pool(tensor)\n",
        "tensor_show(tensor_half_res, \"1/2 down \", ax=ax[1])\n",
        "\n",
        "tensor_q_res, indexes2 = pool(tensor_half_res)\n",
        "tensor_show(tensor_q_res, \"1/4 down \", ax=ax[2])\n",
        "print(\"Downsample shape\", indexes2.shape)\n",
        "\n",
        "# Upsample\n",
        "tensor_half_res1 = unpool(tensor_q_res, indexes2)\n",
        "tensor_show(tensor_half_res1, \"1/2 up \", ax=ax[3])\n",
        "\n",
        "\n",
        "tensor_recovered = unpool(tensor_half_res1, indexes1)\n",
        "tensor_show(tensor_recovered, \"full size up \", ax=ax[4])\n",
        "print(\"Upsample shape\", tensor_recovered.shape)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc0X06V4xxoI"
      },
      "source": [
        "#### Transposed convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbXlwxqhxxoI"
      },
      "source": [
        "Способы восстановления пространственных размерностей, которые мы рассмотрели, не содержали обучаемых параметров.\n",
        "\n",
        "Для повышения пространственного разрешения карты признаков можно использовать операцию *Transposed convolution*, в которой, как в обычной свертке, есть **обучаемые параметры**. Альтернативное название: *Fractionally-strided convolution*.\n",
        "\n",
        "Иногда **некорректно** называется *обратной сверткой* или *Deconvolution*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqboA5JRxxoK"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/simple_convolution.png\" width=\"700\"></center>\n",
        "<center><em>Обычная свертка</em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4F_uTuRxxoK"
      },
      "source": [
        "Операция обычной свертки накладывает фильтр-ядро на фрагмент карты, выполняет поэлементное умножение, а затем сложение, превращая **один фрагмент** входа в **один пиксель** выхода.\n",
        "\n",
        "Transposed convolution, наоборот, проходит по всем пикселям входа и умножает их на **обучаемое ядро** свертки. При этом каждый **одиночный пиксель** превращается в **фрагмент**. Там, где фрагменты накладываются друг на друга, значения попиксельно суммируются.\n",
        "\n",
        "Если вход имеет несколько каналов, то Transposed convolution применяет отдельный обучаемый фильтр к каждому каналу, а результат суммирует.\n",
        "\n",
        "Параметр `stride` отвечает за дополнительный сдвиг каждого фрагмента на выходе. Используя Transposed convolution с параметром `stride = 2`, можно повышать размер карты признаков приблизительно в два раза, добавляя на нее мелкие детали."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap7VFlImxxoL"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/transposed_convolution_explained.png\" width=\"1024\"></center>\n",
        "\n",
        "<center><em>Transposed convolution</em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B0MpRkixxoL"
      },
      "source": [
        "В отличие от обычной свертки, параметр `padding` в Transposed convolution отвечает не за добавление \"рамки\" из нулей по краям изображения/карты признаков для сохранения пространственного разрешения на выходе после свертки, а, наоборот, за удаление внешнего края (\"рамки\") выходной карты признаков. Это может быть полезно, потому что карта признаков (feature map) строится с перекрытием фрагментов, полученных из соседних пикселей, но по периметру результат формируется без перекрытия и может иметь более низкое качество."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xgbe0ErxxoL"
      },
      "source": [
        "Как правило, размер ядра `kernel_size` выбирают кратным `stride`, чтобы избавиться от артефактов-ложных перемножений промежуточных признаков при частичном наложении фрагментов, например:\n",
        "```\n",
        "kernel_size = 4\n",
        "stride = 2\n",
        "```\n",
        "При таких значениях имеет смысл установить `padding=2`, чтобы убрать внешние два пикселя со всех сторон выходной карты признаков, полученные без перекрытия."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4fdK3oQxxoL"
      },
      "source": [
        "[[blog] ✏️ Про 2D свертки с помощью перемножения матриц](https://www.baeldung.com/cs/convolution-matrix-multiplication)\n",
        "\n",
        "[[arxiv] 🎓 A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285v1.pdf) — откуда слово Transposed в названии (раздел 4.1)\n",
        "\n",
        "[[doc] 🛠️](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=transpose#convtranspose2d) `torch.nn.ConvTranspose2d`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6a1yWKfxxoM"
      },
      "source": [
        "```\n",
        "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n",
        "                         stride=1, padding=0, ...)\n",
        "```\n",
        "где:\n",
        "* `in_channels`, `out_channels` — количество каналов во входной и выходной картах признаков,\n",
        "* `kernel_size` — размер ядра свертки Transposed convolution,\n",
        "* `stride` — шаг свертки Transposed convolution,\n",
        "* `padding`— размер отступов, устанавливаемых по краям входной карты признаков.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoofAZ1vxxoN"
      },
      "source": [
        "Пример использования:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CimIlLVYxxoN"
      },
      "outputs": [],
      "source": [
        "input = torch.randn(1, 16, 16, 16)  # define dummy input\n",
        "print(\"Original size\", input.shape)\n",
        "\n",
        "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)  # define downsample layer\n",
        "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)  # define upsample layer\n",
        "\n",
        "# let`s downsample and upsample input\n",
        "with torch.no_grad():\n",
        "    output_1 = downsample(input)\n",
        "    print(\"Downsampled size\", output_1.size())\n",
        "\n",
        "    output_2 = upsample(output_1, output_size=input.size())\n",
        "    print(\"Upsampled size\", output_2.size())\n",
        "\n",
        "# plot results\n",
        "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
        "sns.heatmap(input[0, 0, :, :], ax=ax[0], cbar=False, vmin=-2, vmax=2)\n",
        "ax[0].set_title(\"Input\")\n",
        "sns.heatmap(output_1[0, 0, :, :], ax=ax[1], cbar=False, vmin=-2, vmax=2)\n",
        "ax[1].set_title(\"Downsampled\")\n",
        "sns.heatmap(output_2[0, 0, :, :], ax=ax[2], cbar=False, vmin=-2, vmax=2)\n",
        "ax[2].set_title(\"Upsampled\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnG96TjYxxoO"
      },
      "source": [
        "## Пирамида признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHKWcgqwxxoO"
      },
      "source": [
        "Возникает вопрос: не потеряется ли информация о мелких деталях изображения при передаче через центральный блок сети, где пространственное разрешение минимально? Такая проблема существует.\n",
        "\n",
        "Те, кто изучал классические методы машинного зрения, помнят, что  при извлечении дескрипторов особых точек ([SIFT 📚[wiki]](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform)) использовалась так называемая пирамида изображений."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwG-rDNyxxoO"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/pyramid_of_features.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2slZ1RRxxoP"
      },
      "source": [
        "Идея состоит в последовательном уменьшении (масштабировании) изображения и последовательном извлечении признаков в разных разрешениях.\n",
        "\n",
        "При уменьшении пространственных размеров мы естественным образом получаем карты признаков с разным пространственным разрешением."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNRvWBcFxxoP"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_information.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_4YrCj9xxoP"
      },
      "source": [
        "Их можно использовать одновременно как в качестве входа для новых сверток, так и для получения предсказаний.\n",
        "\n",
        "На этой модели построены FPN-сети.\n",
        "\n",
        "[[arxiv] 🎓 Feature Pyramid Networks for Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1612.03144)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuDa_rPrxxoP"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/fcn_1.png\" width=\"1000\">\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf\"> Fully Convolutional Networks for Semantic Segmentation</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T_gmnomxxoQ"
      },
      "source": [
        "После того, как все карты признаков будут увеличены до одного размера, они суммируются."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5UfMJuFxxoQ"
      },
      "source": [
        "Примеры использования:\n",
        "* [[doc] 🛠️ Fully-Convolutional Network model with ResNet-50 and ResNet-101 backbones](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/)\n",
        "* [[doc] 🛠️ Models and Pre-trained Weights](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
        "\n",
        "\n",
        "Модель была предобучена на части датасета COCO train2017 (на 20 категориях, представленных так же в датасете  Pascal VOC). Использовались следующие классы:\n",
        "\n",
        "`['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4L0lVGtxxoQ"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def coco2pil(url):\n",
        "    print(url)\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "\n",
        "# load resnet50\n",
        "fcn_model = torchvision.models.segmentation.fcn_resnet50(\n",
        "    weights=\"FCN_ResNet50_Weights.DEFAULT\", num_classes=21\n",
        ")\n",
        "\n",
        "classes = [\n",
        "    \"__background__\",\n",
        "    \"aeroplane\",\n",
        "    \"bicycle\",\n",
        "    \"bird\",\n",
        "    \"boat\",\n",
        "    \"bottle\",\n",
        "    \"bus\",\n",
        "    \"car\",\n",
        "    \"cat\",\n",
        "    \"chair\",\n",
        "    \"cow\",\n",
        "    \"diningtable\",\n",
        "    \"dog\",\n",
        "    \"horse\",\n",
        "    \"motorbike\",\n",
        "    \"person\",\n",
        "    \"pottedplant\",\n",
        "    \"sheep\",\n",
        "    \"sofa\",\n",
        "    \"train\",\n",
        "    \"tvmonitor\",\n",
        "]\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        ),  # ImageNet\n",
        "    ]\n",
        ")\n",
        "\n",
        "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
        "input_tensor = transform(pil_img)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = fcn_model(input_tensor.unsqueeze(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9o0U3NZxxoR"
      },
      "source": [
        "Возвращается словарь `output`, в котором по ключу `out` содержится массив со значениями ненормированных вероятностей, соответствующих предсказаниям каждого класса.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GhTTYW9xxoR"
      },
      "outputs": [],
      "source": [
        "print(\"output keys: \", output.keys())  # Ordered dictionary\n",
        "print(\"out: \", output[\"out\"].shape, \"Batch, class_num, h, w\")\n",
        "\n",
        "output_predictions = output[\"out\"][0].argmax(0)  # for first element of batch\n",
        "print(f\"output_predictions: {output_predictions.shape}\")\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.imshow(pil_img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "indexes = output_predictions\n",
        "semantic_seg_person_predict = np.zeros(pil_img.size).astype(bool)\n",
        "\n",
        "# plot all classes predictions\n",
        "fig, ax = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\n",
        "i = 0  # counter\n",
        "for row in range(4):\n",
        "    for col in range(5):\n",
        "        mask = torch.zeros(indexes.shape)\n",
        "        mask[indexes == i] = 255\n",
        "\n",
        "        ax[row, col].set_title(classes[i])\n",
        "        ax[row, col].imshow(mask)\n",
        "        ax[row, col].axis(\"off\")\n",
        "        i += 1\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agm-JSWoxxoR"
      },
      "outputs": [],
      "source": [
        "semantic_seg_person_predict = torch.zeros(indexes.shape)\n",
        "semantic_seg_person_predict[indexes == 15] = 1  # to obtain binary mask\n",
        "semantic_seg_person_predict = (\n",
        "    semantic_seg_person_predict.numpy()\n",
        ")  # for sklearn compability\n",
        "\n",
        "plt.imshow(semantic_seg_person_predict, cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNTP_BFsxxoS"
      },
      "source": [
        "##Метрики"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWWLeK1GxxoS"
      },
      "source": [
        "### IoU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZKUkB-hxxoS"
      },
      "source": [
        "Как оценить качество предсказаний, полученных от модели?\n",
        "\n",
        "Базовой метрикой является Intersection over Union (IoU), она же коэффициент Жаккарда ([Jaccard index 📚[wiki]](https://en.wikipedia.org/wiki/Jaccard_index)).\n",
        "\n",
        "Имеются предсказание модели (фиолетовая маска) и целевая разметка,  сделанная человеком (красная маска)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC6jYcjvxxoS"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/iou_sample.png\" width=\"400\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://datahacker.rs/deep-learning-intersection-over-union/\">Intersection over Union</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXCqUrJzxxoT"
      },
      "source": [
        "Необходимо оценить качество предсказания.\n",
        "\n",
        "**Для простоты в примере маски прямоугольные, но та же логика будет работать  для масок произвольной формы.*\n",
        "\n",
        "Метрика считается как отношение площади пересечения к площади объединения двух масок:\n",
        "\n",
        "$$ \\large \\text{IoU} = \\frac{|T \\cap P|}{|T \\cup P|} $$\n",
        "\n",
        "$T$ — True mask, $P$ — predicted mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT2zcRimxxoT"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/iou_formula.png\" width=\"500\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2nNl4lJxxoU"
      },
      "source": [
        "Если маски совпадут на $100\\%$, то значение метрики будет равно $1$, и это наилучший результат. При пустом пересечении $\\text{IoU} $ будет нулевым. Значения метрики лежат в интервале $[0..1]$.\n",
        "\n",
        "В терминах ошибок первого/второго рода $\\text{IoU}$  можно записать как:\n",
        "\n",
        "$$ \\large \\text{IoU} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP} + \\text{FN}} $$\n",
        "\n",
        "\n",
        "$\\text{TP}$ — True positive — пересечение (обозначено желтым),\n",
        "\n",
        "$\\text{FP}$ — False Positive (остаток фиолетового прямоугольника),\n",
        "\n",
        "$\\text{FN}$ — False Negative (остаток красного прямоугольника)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP7OFcsXxxoU"
      },
      "source": [
        "На базе этой метрики строится ряд производных от нее метрик, таких как Mean Average Precision, которую мы рассмотрим в разделе Детектирование.\n",
        "\n",
        "[[blog] ✏️ Intersection over Union](http://datahacker.rs/deep-learning-intersection-over-union/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agNWxn2oxxoV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"GT mask\")\n",
        "plt.imshow(semantic_seg_person_mask)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Predicted mask\")\n",
        "plt.imshow(semantic_seg_person_predict)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"GT & Predict overlap\")\n",
        "plt.axis(\"off\")\n",
        "tmp = semantic_seg_person_predict * 2 + semantic_seg_person_mask\n",
        "plt.imshow(tmp)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM15c4zKxxoW"
      },
      "source": [
        "Реализации:\n",
        "\n",
        "* [[git] 🐾 Jaccard score в Torchmetrics](https://github.com/Lightning-AI/torchmetrics/blob/8fade87062a7b87c1e6429bbe1c4e0112b3713a5/torchmetrics/functional/classification/jaccard.py)\n",
        "* [[doc] 🛠️ Jaccard score в Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)\n",
        "* [[doc] 🛠️ SMP в PyTorch](https://smp.readthedocs.io/en/latest/metrics.html#segmentation_models_pytorch.metrics.functional.iou_score)\n",
        "* [[doc] 🛠️ Jaccard index between two sets of boxes в Torchvision](https://pytorch.org/vision/main/generated/torchvision.ops.box_iou.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tba1Mmzcxxob"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "# wait vectors, so we flatten the data\n",
        "y_true = semantic_seg_person_mask.flatten()\n",
        "y_pred = semantic_seg_person_predict.flatten()\n",
        "iou = jaccard_score(y_true, y_pred)\n",
        "\n",
        "print(f\"IoU = {iou:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyDs0fMlxxob"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
        "# or use:  smp.metrics.get_stats\n",
        "# https://smp.readthedocs.io/en/latest/metrics.html#segmentation_models_pytorch.metrics.functional.get_stats\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "iou = tp / (tp + fp + fn)\n",
        "print(f\"IoU = {iou:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjtQmIRWxxoc"
      },
      "source": [
        "### Dice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp0eFvQHxxoc"
      },
      "source": [
        "Другой популярной метрикой для оценки качества сегментации является $\\text{Dice}$ коэффициент:\n",
        "\n",
        "$$ \\large \\text{Dice} = \\dfrac{2|A \\cap B|}{|A| + |B|} $$\n",
        "\n",
        "Концептуально он похож на $\\text{IoU}$, но при выражении через ошибки первого и второго рода будет видно, что он совпадет с [F1-мерой 🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html):\n",
        "\n",
        "$$ \\large \\text{Dice} =  \\frac{2|A \\cap B|}{|A| + |B|} = \\dfrac{2\\text{TP}}{2\\text{TP} + \\text{FP} + \\text{FN}} = \\text{F1_score} ∈ [0,1]$$\n",
        "\n",
        "[[blog] ✏️ F1 Score = Dice Coefficient](https://chenriang.me/f1-equal-dice-coefficient.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpzjJJllxxoc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "dice_smp = f1_score(y_true, y_pred)\n",
        "dice = 2 * tp / (2 * tp + fp + fn)\n",
        "print(f\"Dice = {dice:.2f}\")\n",
        "print(f\"F1_score = {dice_smp:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cOZXcAFxxoc"
      },
      "source": [
        "Как видно, $\\text{IoU}$ сильнее наказывает за ошибки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp8G0urlxxod"
      },
      "source": [
        "[[blog] ✏️ All the segmentation metrics!](https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9veSGsC7xxod"
      },
      "source": [
        "## Loss функции для сегментации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbXhVz7kxxod"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/loss_overview.png\" width=\"900\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://github.com/JunMa11/SegLoss\"> Loss functions for image segmentation </a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1_So6yUxxod"
      },
      "source": [
        "### Distribution-based loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRiABd-bxxod"
      },
      "source": [
        "Так как задача сегментации сводится к задаче классификации, то можно использовать Cross-Entropy Loss, BCE или Focal Loss, с которыми мы знакомы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II4oOgTXxxoe"
      },
      "source": [
        "#### Binary Cross-Entropy (BCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTNMcSMOxxoe"
      },
      "source": [
        "Если предсказывается маска для объектов единственного класса (`target.shape` = 1×H×W), то задача сводится к бинарной классификации, так как каждый канал на выходе последнего слоя выдает предсказание для единственного класса.\n",
        "\n",
        "Это позволяет заменить Softmax в Cross-Entropy Loss на сигмоиду, а функцию потерь — на бинарную кросс-энтропию (BCE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgsNae9Gxxoe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "mask_class_1 = torch.randint(0, 2, (1, 64, 64))  # [0, 1]\n",
        "one_class_out = torch.randn(1, 1, 64, 64)\n",
        "print(mask_class_1.shape)\n",
        "print(one_class_out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QywMy9LMxxoe"
      },
      "source": [
        "Применяем [BCE with Logits Loss 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXa9xrKlxxof"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "bce_loss_wl = nn.BCEWithLogitsLoss()  # Sigmoid inside\n",
        "loss = bce_loss_wl(\n",
        "    one_class_out, mask_class_1.float().unsqueeze(0)\n",
        ")  # both params must have equal size\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLjMOBooxxof"
      },
      "source": [
        "Если последний слой модели — это Сигмоида, то можем использовать [BCE Loss 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un13ga-Exxof"
      },
      "outputs": [],
      "source": [
        "norm_one_class_out = one_class_out.sigmoid()\n",
        "\n",
        "bce_loss = nn.BCELoss()\n",
        "loss = bce_loss(\n",
        "    norm_one_class_out, mask_class_1.float().unsqueeze(0)\n",
        ")  # both params must have equal size\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzHrgsVzxxof"
      },
      "source": [
        "[Cross-Entropy Loss 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) для одного класса работать не будет:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11_p8YLWxxog"
      },
      "outputs": [],
      "source": [
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "loss = cross_entropy(one_class_out, mask_class_1.float().unsqueeze(0))\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-vx_yf9xxog"
      },
      "source": [
        "Так как Softmax от единственного входа всегда равен $1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjBxU_p4xxog"
      },
      "outputs": [],
      "source": [
        "one_class_out.softmax(dim=1).unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRAyjlKlxxoh"
      },
      "source": [
        "Эту проблему можно решить искусственно, добавив в маску второй класс фона (background) отдельным каналом. Иными словами, для использования `CrossEntropyLoss` с `Softmax` в конце архиектуры нужно сделать One hot encoding одноканальной маски [OHE 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html). Тем самым, задача примет вид Multiclass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRunfJN5xxoh"
      },
      "source": [
        "**Multilabel**\n",
        "\n",
        "Если предсказываются несколько классов и `target` имеет форму N×W×H (multilabel), где N — количество классов, то маска каждого хранится в отдельном канале:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5YUA9fRxxoi"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_mask2.png\" width=\"600\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.jeremyjordan.me/semantic-segmentation/\"> An overview of semantic image segmentation </a></em></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmQyh3hsxxoi"
      },
      "outputs": [],
      "source": [
        "mask_class1 = torch.randint(0, 2, (1, 64, 64))  # [0 , 1]\n",
        "mask_class2 = torch.randint(0, 2, (1, 64, 64))\n",
        "\n",
        "target = torch.cat((mask_class1, mask_class2))\n",
        "\n",
        "print(target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srrj5mQBxxoi"
      },
      "source": [
        "Видим, что форма выхода модели совпадает с формой тензора масок:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kltEYWJTxxoi"
      },
      "outputs": [],
      "source": [
        "two_class_out = torch.randn(1, 2, 64, 64)\n",
        "print(two_class_out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb4lqskCxxoj"
      },
      "source": [
        "Мы можем посчитать [BCE 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) поэлементно, предварительно преобразовав `target` во `float`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-R2avjPxxoj"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
        "\n",
        "bce_loss = nn.BCEWithLogitsLoss()  # Sigmoid inside\n",
        "float_target = target.float()  # add batch and convert ot float\n",
        "loss = bce_loss(\n",
        "    two_class_out, float_target.unsqueeze(0)\n",
        ")  # both params must have equal size\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtf908H5xxoj"
      },
      "source": [
        "Или [Cross-Entropy Loss 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y022P9Caxxoj"
      },
      "outputs": [],
      "source": [
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "# If containing class probabilities, same shape as the input and each value should be between [0,1][0,1].\n",
        "loss = cross_entropy(two_class_out, float_target.unsqueeze(0))\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV1Qq71kxxok"
      },
      "source": [
        "Результаты не совпадают, так как после Sigmoid и Softmax получаются разные вероятности.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gr7PlL1xxok"
      },
      "source": [
        "#### Cross-Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjnvCVxexxok"
      },
      "source": [
        "Если маска задана одним каналом, в котором классы пронумерованы целыми числами (multiclass):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS1iWpQYxxol"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_mask1.png\" width=\"900\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.jeremyjordan.me/semantic-segmentation/\"> An overview of semantic image segmentation </a></em></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyOja5uBxxol"
      },
      "outputs": [],
      "source": [
        "sq_target = target.argmax(0)\n",
        "sq_target.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHKJYChFxxol"
      },
      "source": [
        "То логично использовать `nn.CrossEntropyLoss`:\n",
        "\n",
        "```\n",
        "Input: Shape (C), (N,C) or (N,C,d1​,d2​,...,dK​) with K≥1 in the case of K-dimensional loss.\n",
        "\n",
        "Target: If containing class indices, shape (), (N) or (N,d1,d2,...,dK)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2ZiIs3fxxol"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "loss = cross_entropy(two_class_out, sq_target.unsqueeze(0))\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSLhbA_cxxom"
      },
      "source": [
        "**Различия Multilabel и Multiclass**\n",
        "\n",
        "Важно отметить, что помимо способа хранения/записи масок, режим multilabel подразумевает возможность принадлежности одного пикселя сразу нескольким классам, т. е. пересечение классов, что недопустимо в режиме multiclass.\n",
        "\n",
        "Таким образом, задача Multilabel является более сложной для нейросети. В библитеке Segmentation models PyTorch, рассматриваемой ниже, в функциях потерь для сегментации уже реализованы режимы **Binary, Multilabel и Multiclass** [SMP Losses 🛠️[doc]](https://segmentation-models-pytorch.readthedocs.io/en/latest/losses.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXYogLv6xxom"
      },
      "source": [
        "### Region-based loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_PEJutPxxom"
      },
      "source": [
        "К Region-based loss относятся функции потерь, основанные на оценке площади пересечения масок.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz6yikVNxxom"
      },
      "source": [
        "#### Jaccard Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZmhktjSxxon"
      },
      "source": [
        "В отличие от accuracy, рассчет IoU (Jaccard index):\n",
        "\n",
        "$\\large \\text{IoU} = \\text{JaccardIndex} = \\dfrac{  TP  }{TP + FP + FN} \\in [0,1]$\n",
        "\n",
        "можно произвести [дифференцируемым образом 🎓[arxiv]](https://arxiv.org/abs/1908.03851).\n",
        "\n",
        "Для этого нужно применить к предсказаниям функцию Sigmoid, а операцию пересечения заменить поэлементным умножением:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8V4BLh9xxon"
      },
      "source": [
        "$$ \\large \\text{IoU} = \\frac{|A \\cap B|}{|A \\cup B|} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8IUT1PTxxon"
      },
      "source": [
        "$ |A \\cap B| =\n",
        "\\begin{bmatrix}\n",
        "0.01 & 0.03 & 0.02 & 0.02 \\\\ 0.05 & 0.12 & 0.09 & 0.07 \\\\ 0.89 & 0.85 & 0.88 & 0.91 \\\\ 0.99 & 0.97 & 0.95 & 0.97 \\end{bmatrix} * \\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}\n",
        "\\xrightarrow{\\ element-wise \\ multiply \\ } \\begin{bmatrix}\n",
        "0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0.89 & 0.85 & 0.88 & 0.91 \\\\ 0.99 & 0.97 & 0.95 & 0.97 \\end{bmatrix} \\xrightarrow{\\ sum \\ } 7.41$\n",
        "\n",
        "$\\qquad \\qquad \\qquad \\qquad \\text{prediction} \\qquad \\qquad \\qquad \\quad \\text{target}$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwviRiNUxxon"
      },
      "source": [
        "$ |A| =\n",
        "\\begin{bmatrix}\n",
        "0.01 & 0.03 & 0.02 & 0.02 \\\\ 0.05 & 0.12 & 0.09 & 0.07 \\\\ 0.89 & 0.85 & 0.88 & 0.91 \\\\ 0.99 & 0.97 & 0.95 & 0.97 \\end{bmatrix}\n",
        "\\xrightarrow{\\ sum \\ }  7.82 \\qquad \\qquad|B| =\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\xrightarrow{\\ sum \\ }  8 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgsifzZwxxon"
      },
      "source": [
        "$|A \\cup B| = |A +B - A  \\cap B|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB-fYI-Exxoo"
      },
      "source": [
        "И тогда метрику можно превратить в функцию потерь, инвертировав ее:\n",
        "\n",
        "$\\large \\text{Jaccard Loss} = 1 - \\text{IoU}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zf-S3doxxoo"
      },
      "source": [
        "В PyTorch такой loss не реализован, поэтому для его использования установим библиотеку [SMP 🐾[git]](https://github.com/qubvel/segmentation_models.pytorch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_HlYonMxxoo"
      },
      "outputs": [],
      "source": [
        "!pip install -q segmentation-models-pytorch\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0nxh7a0xxoo"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "iou_loss = smp.losses.JaccardLoss(smp.losses.MULTILABEL_MODE, from_logits=True)\n",
        "print(\"IoU Loss\", iou_loss(two_class_out, float_target.unsqueeze(0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZLW7q43xxop"
      },
      "source": [
        "* При наличии на выходе модели сигмоиды или softmax, параметр `from_logits` следует устaновить равным `False`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc-vopNuxxop"
      },
      "source": [
        "#### Dice loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kupBzrbLxxop"
      },
      "source": [
        "Аналогично можно посчитать Dice коэффициент:\n",
        "\n",
        "$$ \\large \\text{Dice} = \\dfrac{2|A \\cap B|}{|A| + |B|} $$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH2EBTgoxxop"
      },
      "source": [
        "$$ \\large \\text{Dice} = \\frac{2 \\sum\\limits_\\text{pixels}y_\\text{true}y_\\text{pred}}{\\sum\\limits_\\text{pixels}y_\\text{true} + \\sum\\limits_\\text{pixels}y_\\text{pred}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymZXQFNaxxop"
      },
      "source": [
        "И затем превратить в функцию потерь:\n",
        "\n",
        "$$ \\large \\text{DiceLoss} = 1 - \\text{Dice} = 1 - \\dfrac{2\\sum\\limits_\\text{pixels}y_\\text{true}y_\\text{pred}}{\\sum\\limits_\\text{pixels}y_\\text{true} + \\sum\\limits_\\text{pixels}y_\\text{pred}} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvOVEthUxxop"
      },
      "source": [
        "Эта функция потерь также отсутствует в PyTorch, поэтому воспользуемся библиотекой [SMP 🐾[git]](https://github.com/qubvel/segmentation_models.pytorch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPDDoYR1xxoq"
      },
      "outputs": [],
      "source": [
        "dice_loss = smp.losses.DiceLoss(smp.losses.MULTILABEL_MODE, from_logits=True)\n",
        "\n",
        "print(two_class_out.shape, target.shape)\n",
        "print(\"DICE Loss\", dice_loss(two_class_out, float_target.unsqueeze(0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmxONYZDxxoq"
      },
      "source": [
        "Концептуально Dice Loss и Jaccard Loss похожи. Но Jaccard Loss [сильнее штрафует модель на выбросах ✏️[blog]](https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yJOxTadxxoq"
      },
      "source": [
        "### Другие loss функции для сегментации и их различия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CIAVfX3xxoq"
      },
      "source": [
        "Основная часть функций потерь, относящихся к типам Region-based и Distribution-based, для задач семантической сегментации реализована в [SMP Losses 🛠️[doc]](https://segmentation-models-pytorch.readthedocs.io/en/latest/losses.html). Отметим некоторые эмпирические соображения:\n",
        "\n",
        "* Все Distribution-based losses представляют собой производные от cross entropy loss, что зачастую позволяет модели более стабильно обучаться.\n",
        "\n",
        "* Region-based losses являются дифференцируемыми аналогами основных метрик для оценки качества сегментации, поэтому использование их при обучении обеспечивает максимизацию нужных нам метрик, а именно Dice, IoU.\n",
        "\n",
        "* На практике, для обеспечения преимуществ каждого из типов функций потерь, их часто используют в линейной комбинации (Compound Loss):\n",
        "$$ \\text{SegLoss} = \\beta\\times \\text{RegionBasedLoss} + (1-\\beta) \\times \\text{DistributionBasedLoss},  \\quad \\beta \\in [0,1],$$\n",
        "при этом обычно весовой коэффициент $\\beta$ задают небольшим, отдавая приоритет DistributionBasedLoss.\n",
        "\n",
        "* Существуют также Boundary-based segmentation losses. Они используются при необходимости сохранить наиболее точные границы областей при сегментации, но на практике используются редко.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as5sSSKVxxor"
      },
      "source": [
        "Хороший обзор функций потерь для семантической сегментации можно найти в [Loss functions for image segmentation 🐾[git]](https://github.com/JunMa11/SegLoss)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZhqmUZVxxor"
      },
      "source": [
        "## U-Net: Convolutional Networks for Biomedical Image Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_w8p93Pxxos"
      },
      "source": [
        "Можно спроектировать сеть так, что карты признаков на выходе сжимающих и соответствующих им разжимающих блоков будут иметь одинаковый размер."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWDUZvz4xxos"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/efficient_way_predict_pixel_class.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNARYUSzxxos"
      },
      "source": [
        "Признаки, полученные при сжатии, скопируем и объединим с признаками в разжимающих слоях, где карты признаков имеют соответствующее пространственное разрешение:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNO7JjDtxxos"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/add_skip_connection.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHIjRLJYxxos"
      },
      "source": [
        "Так же, как и в ResNet, этот механизм носит название skip connection, но  признаки  не суммируются, а конкатенируются."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceJkiFpfxxot"
      },
      "source": [
        "Рассмотренная нами схема используется в U-Net. Эта популярная модель для сегментации медицинских изображений изначально была предложена в [статье 🎓[arxiv]](https://arxiv.org/abs/1505.04597) для анализа  медицинских изображений."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXCFAvTjxxot"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/unet_scheme.png\" width=\"700\"></center>\n",
        "\n",
        "<center><em>Архитектура U-Net</em></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://arxiv.org/abs/1505.04597\">U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDhL5ozjxxot"
      },
      "source": [
        "[[git] 🐾 Реализация на PyTorch](https://github.com/milesial/Pytorch-UNet)\n",
        "\n",
        "[[doc] 🛠️ U-Net на PyTorch Hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUEWLkpnxxot"
      },
      "source": [
        "Стоит обратить особое внимание на серые стрелки на схеме: они соответствуют операции конкатенации копий ранее полученных карт активаций по аналогии с DenseNet. Чтобы это было возможно, необходимо поддерживать соответствие между размерами карт активаций в процессах снижения и повышения пространственных размерностей. Для этой цели изменения размеров происходят только при операциях `MaxPool` и `MaxUnpool` — в обоих случаях в два раза."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFyJqhPVxxou"
      },
      "source": [
        "В коде прямой проход может быть реализован, например, вот так:\n",
        "\n",
        "```\n",
        "def forward(self, x):\n",
        "    out1 = self.block1(x) #  ------------------------------>\n",
        "    out_pool1 = self.pool1(out1)\n",
        "\n",
        "    out2 = self.block2(out_pool1)\n",
        "    out_pool2 = self.pool2(out2)\n",
        "\n",
        "    out3 = self.block3(out_pool2)\n",
        "    out_pool3 = self.pool2(out3)\n",
        "\n",
        "    out4 = self.block4(out_pool3)\n",
        "    # return up\n",
        "    out_up1 = self.up1(out4)\n",
        "\n",
        "    out_cat1 = torch.cat((out_up1, out3), dim=1)\n",
        "    out5 = self.block5(out_cat1)\n",
        "    out_up2 = self.up2(out5)\n",
        "\n",
        "    out_cat2 = torch.cat((out_up2, out2), dim=1)\n",
        "    out6 = self.block6(out_cat2)\n",
        "    out_up3 = self.up3(out6)\n",
        "\n",
        "    out_cat3 = torch.cat((out_up3, out1), dim=1) # <-------\n",
        "    out = self.block7(out_cat3)\n",
        "\n",
        "    return out\n",
        "\n",
        "```\n",
        "После Upsample-блоков ReLU не используется."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EB9CZSSxxou"
      },
      "source": [
        "## Обзор DeepLabv3+ (2018)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efIzSZ5dxxou"
      },
      "source": [
        "DeepLab — семейство моделей для сегментации, значительно развивавшееся в течение четырёх лет. Основой данного рода моделей является использование **atrous (dilated) convolutions** и, начиная со второй модели, **atrous spatial pyramid pooling**, опирающейся на **spatial pyramid pooling**.\n",
        "\n",
        "[[arxiv] 🎓 Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Chen et al., 2018)](https://arxiv.org/abs/1802.02611v3)\n",
        "\n",
        "[[doc] 🛠️ Реализация на PyTorch](https://pytorch.org/vision/stable/models.html#deeplabv3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCQgbeiexxou"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/deeplabv3_scheme.png\" width=\"800\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.researchgate.net/publication/339754839_Semantic_Image_Segmentation_with_Deep_Convolutional_Neural_Networks_and_Quick_Shift\">Semantic Image Segmentation with DeepConvolutional Neural Networks and Quick Shift</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0npKM8i8xxov"
      },
      "source": [
        "### Atrous (Dilated) Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvj1E6gExxov"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/dilated_convolution.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkrko_qlxxov"
      },
      "source": [
        "**Dilated convolution** (расширенная свертка) — это тип свертки, который \"раздувает\" ядро, как бы вставляя отверстия между элементами ядра, т. е. делает его разреженным. Дополнительный параметр `dilation` указывает, насколько сильно расширяется ядро.\n",
        "\n",
        "Фактически в такой свертке входные пиксели (признаки) участвуют через один (два, три ...). Параметр `dilation` указывает, какой по счету пиксель брать про свертке с ядром: при `dilation = 1` — каждый первый, при `dilation = 2` — каждый второй и т. д.\n",
        "\n",
        "Расширенные свертки позволяют значительно увеличить рецептивное поле и хорошо показывают себя при [решении задач семантической сегментации изображений 🎓[arxiv]](https://arxiv.org/pdf/1511.07122.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Foh3nE6xxow"
      },
      "source": [
        "\n",
        "```\n",
        "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1,\n",
        "                padding=0, dilation=1, ...)\n",
        "```\n",
        "где:\n",
        "* `in_channels`, `out_channels` — количество каналов во входной и выходной картах признаков,\n",
        "* `kernel_size` — размер ядра свертки,\n",
        "* `stride` — шаг свертки,\n",
        "* `padding` — размер отступов, устанавливаемых по краям входной карты признаков,\n",
        "* `dilation` — скорость расширения свертки.\n",
        "\n",
        "[[doc] 🛠️](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) `nn.Conv2d`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_F71eUnu9Xx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from pycocotools.coco import COCO\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def coco2pil(url):\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "\n",
        "coco = COCO(\"annotations/instances_val2017.json\")\n",
        "clear_output()\n",
        "\n",
        "annIds = coco.getAnnIds(imgIds=[448263])\n",
        "anns = coco.loadAnns(annIds)\n",
        "img = coco.loadImgs(anns[0][\"image_id\"])[0]\n",
        "I = coco2pil(img[\"coco_url\"])\n",
        "\n",
        "semantic_seg_person_mask = np.zeros(I.size[::-1], dtype=bool)  # WxH -> HxW\n",
        "\n",
        "for ann in anns:\n",
        "    msk = coco.annToMask(ann)  # HxW\n",
        "    if ann[\"category_id\"] == 1 and not ann[\"iscrowd\"]:  # single person:\n",
        "        # semantic_seg_person_mask = msk | semantic_seg_person_mask  # union\n",
        "        semantic_seg_person_mask += msk.astype(bool)\n",
        "\n",
        "semantic_seg_person_mask = semantic_seg_person_mask > 0  # binarize\n",
        "plt.imshow(semantic_seg_person_mask, cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJJzWqMQu9Xy"
      },
      "source": [
        "## Способы предсказания класса для каждого пикселя"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMYQ1HPuu9Xy"
      },
      "source": [
        "Давайте подумаем о том, как такую задачу можно решить.\n",
        "\n",
        "Из самой постановки задачи видно, что это задача классификации. Только не\n",
        "всего изображения, а каждого пикселя."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDYIvAkLu9Xz"
      },
      "source": [
        "**a) Наивный**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmWQY8Bmu9Xz"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Простейшим вариантом решения является использование так называемого \"скользящего окна\" — последовательное рассмотрение фрагментов изображения. В данном случае интересующими фрагментами будут небольшие зоны, окружающие каждый из пикселей изображения. К каждому из таких фрагментов применяется свёрточная нейронная сеть, предсказывающая, к какому классу относится центральный пиксель."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5isBMR3ju9Xz"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/naive_way_predict_pixel_class.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJsTNGGWu9X0"
      },
      "source": [
        "**б) Разумный**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrJzlicou9X0"
      },
      "source": [
        "Понятно, что запускать классификатор для каждого пикселя абсолютно неэффективно, так как для одного изображения потребуется $H*W$ запусков.\n",
        "\n",
        "Можно пойти другим путем: получить карту признаков и по ней делать предсказание для всех пикселей разом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-rxp2V2u9X1"
      },
      "source": [
        "Для этого потребуется поменять привычную нам архитектуру сверточной сети следующим образом:\n",
        "\n",
        "* убрать слои, уменьшающие пространственные размеры;\n",
        "* убрать линейный слой в конце, заменив его сверточным."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuQx1_B7u9X1"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/reasonable_way_predict_pixel_class.png\" width=\"900\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Fo9OoTu9X2"
      },
      "source": [
        "Теперь пространственные размеры выхода $(W, H)$ будут равны ширине и высоте исходного изображения.\n",
        "\n",
        "Количество выходных каналов будет равно количеству классов, которые мы учимся предсказывать."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQpmdenYu9X2"
      },
      "source": [
        "Тогда можно использовать значения каждой из карт активаций на выходе последнего слоя сети как ненормированное значение вероятности принадлежности (score) каждого из пикселей к тому или иному классу.\n",
        "\n",
        "То есть номер канала с наибольшим значением будет соответствовать классу объекта, который изображает данный пиксель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovrFDDGhu9X2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "last_layer_output = torch.randn((3, 32, 32))  # class_num, W,H\n",
        "print(\"Output of last layer shape\", last_layer_output.shape)  # activation slice\n",
        "mask = torch.argmax(last_layer_output, dim=0)  # class_nums prediction\n",
        "print(\"One class mask shape\", mask.shape)\n",
        "print(\"Predictions for all classes \\n\", mask[:5, :5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2s7AFDLu9X2"
      },
      "source": [
        "Target в этом случае может выглядеть так:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAZqstPTu9X2"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_mask2.png\" width=\"600\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YseedRmDu9X3"
      },
      "source": [
        "Чтобы на выходе сети получить количество каналов, равное количеству классов, используется свертка 1×1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtf1UWinu9X3"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/1x1_kernel_size_fully_connected_layer.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y5NPVNzu9X4"
      },
      "source": [
        "В лекции про сверточные сети мы говорили о том, что свертку 1×1 можно рассматривать как аналог полносвязного слоя. Именно так она тут и работает.\n",
        "\n",
        "**Проблемы:**\n",
        "- чтобы рецептивное поле нейронов на последних слоях было сопоставимо с размером изображения, требуется много сверточных слоев ($L$ раз свёртка $3\\times3$ $\\to$ рецептивное поле $(1+2L)$);\n",
        "- свертки медленно работают на полноразмерных картах активации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIYO-E2du9X4"
      },
      "source": [
        "**в) Эффективный**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gntSNe0Du9X4"
      },
      "source": [
        "Используем стандартную сверточную сеть, но полносвязные слои заменим на сверточные."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLj5aJd_u9X4"
      },
      "source": [
        "## Fully Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH5VRVeBu9X4"
      },
      "source": [
        "[[arxiv] 🎓 Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211)\n",
        "\n",
        "\n",
        "Сокращенно FCN. Для того, чтобы не было путаницы с Fully Connected Network, последние именуют MLP (Multilayer Perceptron).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4y9ZFOeu9X5"
      },
      "source": [
        "За основу берется обычная сверточная сеть для классификации:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1jadeRLu9X5"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/fcn_backbone.png\" width=\"500\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.v7labs.com/blog/semantic-segmentation-guide\">The Beginner’s Guide to Semantic Segmentation</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZOHluX8u9X5"
      },
      "source": [
        "Такую сеть можно построить, взяв за основу другую сверточную архитектуру (*backbone*), например, ResNet50 или VGG16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0i9PJrCu9X6"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/fcn_changes.png\" width=\"500\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.v7labs.com/blog/semantic-segmentation-guide\">The Beginner’s Guide to Semantic Segmentation</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0upJ0l5Gu9X6"
      },
      "source": [
        "И затем заменить полносвязные слои на свертки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMt669fqu9X6"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/fully_convolution_network_scheme.png\" width=\"500\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://arxiv.org/pdf/1411.4038.pdf\"> Fully Convolutional Networks for Semantic Segmentation\n",
        "</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y4HHMrcu9X7"
      },
      "source": [
        "В конце добавить слой `nn.Upsample` до нужных нам размеров.\n",
        "\n",
        "На вход такая модель может получать изображение произвольного размера.\n",
        "Для задач сегментации изменение размеров входного изображения приводит к потере важной информации о границах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvp_KjJ0u9X7"
      },
      "source": [
        "## Разжимающий слой"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKY4Gvm9u9X7"
      },
      "source": [
        "Как реализовать декодировщик?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0Ed4-6hu9X7"
      },
      "source": [
        "### Интерполяция при увеличении разрешения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2HZfKkFu9X8"
      },
      "source": [
        "Вспомним, как повышают разрешение для обычных изображений, а уже затем перейдем к картам признаков.\n",
        "\n",
        "Допустим, требуется увеличить изображение размером 2×2 до размера 4×4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STYGG11pu9X8"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/upsample.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbwzOSTuu9X8"
      },
      "source": [
        "<center><img src=\"https://ml.gan4x4.ru/msu/dep-2.1/L11/comparison_of_1d_and_2d_interpolation.png\" width=\"800\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://en.wikipedia.org/wiki/Bilinear_interpolation\"> Bilinear_interpolation</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp7Bp-Neu9X9"
      },
      "source": [
        "Если для интерполяции используются значения четырех соседних пикселей, то такая интерполяция называется билинейной. В качестве интерполированного значения используется взвешенное среднее этих четырёх пикселей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eEQhBlhu9X9"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def img_to_heatmap(img, ax, title):  # Magic method to show img as heatmap\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title)\n",
        "    array = np.array(img)\n",
        "    array = array[None, None, :]\n",
        "    sns.heatmap(array[0][0], annot=True, ax=ax, lw=1, cbar=False)\n",
        "\n",
        "\n",
        "# Fake image\n",
        "raw = np.array([[1, 3, 0, 1], [3, 3, 3, 7], [8, 1, 8, 7], [6, 1, 1, 1]], dtype=np.uint8)\n",
        "pil = Image.fromarray(raw)\n",
        "\n",
        "interp_nn = pil.resize((8, 8), resample=Image.NEAREST)\n",
        "interp_bl = pil.resize((8, 8), resample=Image.BILINEAR)\n",
        "\n",
        "# Plot result\n",
        "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
        "img_to_heatmap(raw, ax[0], \"Raster dataset\")\n",
        "img_to_heatmap(interp_nn, ax[1], \"Nearest neighbor interpolation\")\n",
        "img_to_heatmap(interp_bl, ax[2], \"Bilinear interpolation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrRD3bOhu9X9"
      },
      "source": [
        "[Билинейная интерполяция 📚[wiki]](https://en.wikipedia.org/wiki/Bilinear_interpolation) позволяет избавиться от резких границ, которые возникают при увеличении методом ближайшего соседа.  Существуют и другие виды интерполяции, использующие большее количество соседних пикселей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY7Q0Q44u9X9"
      },
      "source": [
        "### Upsample в PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-hWwu__u9X-"
      },
      "source": [
        "К чему был этот разговор об увеличении картинок?\n",
        "\n",
        "Оказывается, для увеличения пространственного разрешения карт признаков (feature maps) можно применять те же методы, что и для изображений.\n",
        "\n",
        "Для увеличения пространственного разрешения карт признаков (карт активаций) в PyTorch используется класс `nn.Upsample` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html). В нём доступны все упомянутые методы интерполяции, а также трилинейная интерполяция — аналог билинейной интерполяции, используемый для работы с трёхмерными пространственными данными (к примеру, видео)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut992oypu9X-"
      },
      "source": [
        "[[doc] 🛠️](https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html?highlight=interp#torch.nn.functional.interpolate) `torch.nn.functional.interpolate`\n",
        "\n",
        "Таким образом мы можем использовать `nn.Upsample` внутри разжимающего блока.\n",
        "\n",
        "Загрузим изображение:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mRZupdwu9X-"
      },
      "outputs": [],
      "source": [
        "!wget -q https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_segmentation_1.png -O cat.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjMNntL6u9X-"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "def upsample(pil, ax, mode=\"nearest\"):\n",
        "    tensor = TF.to_tensor(pil)\n",
        "    # Create upsample instance\n",
        "\n",
        "    if mode == \"nearest\":\n",
        "        upsampler = nn.Upsample(scale_factor=2, mode=mode)\n",
        "    else:\n",
        "        upsampler = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
        "\n",
        "    tensor_128 = upsampler(tensor.unsqueeze(0))  # add batch dimension\n",
        "    # Convert tensor to Pillow\n",
        "    img_128 = tensor_128.squeeze()\n",
        "    img_128_pil = TF.to_pil_image(img_128.clamp(min=0, max=1))\n",
        "    ax.imshow(img_128_pil)\n",
        "    ax.set_title(mode)\n",
        "\n",
        "\n",
        "# Load and show image in Pillow format\n",
        "pic = Image.open(\"cat.png\")\n",
        "pil_64 = pic.resize((64, 64))\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(15, 5))\n",
        "ax[0].imshow(pil_64)\n",
        "ax[0].set_title(\"Raw\")\n",
        "\n",
        "# Upsample with Pytorch\n",
        "upsample(pil_64, mode=\"nearest\", ax=ax[1])\n",
        "upsample(pil_64, mode=\"bilinear\", ax=ax[2])\n",
        "upsample(pil_64, mode=\"bicubic\", ax=ax[3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj3xTt73u9X_"
      },
      "source": [
        "Обратите внимание на то, что в данном случае каждое из пространственных измерений изображения увеличилось в 2 раза, но при необходимости возможно использовать увеличение в иное, в том числе не целое количество раз, используя параметр `scale_factor`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbmsMaGXu9X_"
      },
      "source": [
        "Слои `nn.Upsample` обычно комбинируют вместе со сверточными, это рекомендованный способ увеличения пространственных размеров карт признаков."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHzd_vrDu9YA"
      },
      "outputs": [],
      "source": [
        "# fmt: off\n",
        "model = nn.Sequential(\n",
        "    nn.Upsample(scale_factor=2),\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "    nn.ReLU()\n",
        ")\n",
        "# fmt: on\n",
        "\n",
        "dummy_input = torch.randn((0, 3, 32, 32))\n",
        "out = model(dummy_input)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkPrVj8yu9YA"
      },
      "source": [
        "### Другие способы \"разжать\" карту признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jfYVB7mu9YA"
      },
      "source": [
        "#### MaxUnpooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7TwIX32u9YA"
      },
      "source": [
        "Помимо свёртки, на этапе снижения размерности также используются слои субдискретизации (pooling). Наиболее популярным вариантом является MaxPooling, сохраняющий значение только наибольшего элемента внутри сегмента. Для того, чтобы обратить данную операцию, был предложен слой MaxUnpooling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y-arG2Mu9YA"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/maxunpooling.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9CeS1Tuu9YB"
      },
      "source": [
        "Данный слой требует сохранения индексов максимальных элементов внутри сегментов: при обратной операции максимальное значение помещается на место, в котором был максимальный элемент сегмента до соответствующей субдискретизации. Соответственно, каждому слою MaxUnpooling должен соответствовать слой MaxPooling, что визуально можно представить следующим образом:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_c1Xo_tu9YC"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/downsample_and_upsample_layers.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se6s80Pru9YD"
      },
      "source": [
        "[[doc] 🛠️](\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) `torch.nn.MaxPool2d`\n",
        "\n",
        "[[doc] 🛠️](\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=unpooling) `torch.nn.MaxUnpool2d`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVtzmLG2u9YD"
      },
      "outputs": [],
      "source": [
        "torch.use_deterministic_algorithms(False, warn_only=False)\n",
        "\n",
        "\n",
        "def coco2pil(url):\n",
        "    print(url)\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "\n",
        "def tensor_show(tensor, title=\"\", ax=ax):\n",
        "    img = TF.to_pil_image(tensor.squeeze()).convert(\"RGB\")\n",
        "    ax.set_title(title + str(img.size))\n",
        "    ax.imshow(img)\n",
        "\n",
        "\n",
        "pool = nn.MaxPool2d(\n",
        "    kernel_size=2, return_indices=True\n",
        ")  # False by default(get indexes to upsample)\n",
        "unpool = nn.MaxUnpool2d(kernel_size=2)\n",
        "\n",
        "pil = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
        "\n",
        "fig, ax = plt.subplots(ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
        "\n",
        "ax[0].set_title(\"original \" + str(pil.size))\n",
        "ax[0].imshow(pil)\n",
        "tensor = TF.to_tensor(pil).unsqueeze(0)\n",
        "print(\"Orginal shape\", tensor.shape)\n",
        "\n",
        "# Downsample\n",
        "tensor_half_res, indexes1 = pool(tensor)\n",
        "tensor_show(tensor_half_res, \"1/2 down \", ax=ax[1])\n",
        "\n",
        "tensor_q_res, indexes2 = pool(tensor_half_res)\n",
        "tensor_show(tensor_q_res, \"1/4 down \", ax=ax[2])\n",
        "print(\"Downsample shape\", indexes2.shape)\n",
        "\n",
        "# Upsample\n",
        "tensor_half_res1 = unpool(tensor_q_res, indexes2)\n",
        "tensor_show(tensor_half_res1, \"1/2 up \", ax=ax[3])\n",
        "\n",
        "\n",
        "tensor_recovered = unpool(tensor_half_res1, indexes1)\n",
        "tensor_show(tensor_recovered, \"full size up \", ax=ax[4])\n",
        "print(\"Upsample shape\", tensor_recovered.shape)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5f41fkPu9YE"
      },
      "source": [
        "#### Transposed convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf2P52Tpu9YF"
      },
      "source": [
        "Способы восстановления пространственных размерностей, которые мы рассмотрели, не содержали обучаемых параметров.\n",
        "\n",
        "Для повышения пространственного разрешения карты признаков можно использовать операцию *Transposed convolution*, в которой, как в обычной свертке, есть **обучаемые параметры**. Альтернативное название: *Fractionally-strided convolution*.\n",
        "\n",
        "Иногда **некорректно** называется *обратной сверткой* или *Deconvolution*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVAKqxfuu9YF"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/simple_convolution.png\" width=\"700\"></center>\n",
        "<center><em>Обычная свертка</em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xB-mtWYu9YF"
      },
      "source": [
        "Операция обычной свертки накладывает фильтр-ядро на фрагмент карты, выполняет поэлементное умножение, а затем сложение, превращая **один фрагмент** входа в **один пиксель** выхода.\n",
        "\n",
        "Transposed convolution, наоборот, проходит по всем пикселям входа и умножает их на **обучаемое ядро** свертки. При этом каждый **одиночный пиксель** превращается в **фрагмент**. Там, где фрагменты накладываются друг на друга, значения попиксельно суммируются.\n",
        "\n",
        "Если вход имеет несколько каналов, то Transposed convolution применяет отдельный обучаемый фильтр к каждому каналу, а результат суммирует.\n",
        "\n",
        "Параметр `stride` отвечает за дополнительный сдвиг каждого фрагмента на выходе. Используя Transposed convolution с параметром `stride = 2`, можно повышать размер карты признаков приблизительно в два раза, добавляя на нее мелкие детали."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8YIHkbiu9YF"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/transposed_convolution_explained.png\" width=\"1024\"></center>\n",
        "\n",
        "<center><em>Transposed convolution</em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OME-iUWLu9YF"
      },
      "source": [
        "В отличие от обычной свертки, параметр `padding` в Transposed convolution отвечает не за добавление \"рамки\" из нулей по краям изображения/карты признаков для сохранения пространственного разрешения на выходе после свертки, а, наоборот, за удаление внешнего края (\"рамки\") выходной карты признаков. Это может быть полезно, потому что карта признаков (feature map) строится с перекрытием фрагментов, полученных из соседних пикселей, но по периметру результат формируется без перекрытия и может иметь более низкое качество."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HitDQejru9YF"
      },
      "source": [
        "Как правило, размер ядра `kernel_size` выбирают кратным `stride`, чтобы избавиться от артефактов-ложных перемножений промежуточных признаков при частичном наложении фрагментов, например:\n",
        "```\n",
        "kernel_size = 4\n",
        "stride = 2\n",
        "```\n",
        "При таких значениях имеет смысл установить `padding=2`, чтобы убрать внешние два пикселя со всех сторон выходной карты признаков, полученные без перекрытия."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xetJK5azu9YG"
      },
      "source": [
        "[[blog] ✏️ Про 2D свертки с помощью перемножения матриц](https://www.baeldung.com/cs/convolution-matrix-multiplication)\n",
        "\n",
        "[[arxiv] 🎓 A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285v1.pdf) — откуда слово Transposed в названии (раздел 4.1)\n",
        "\n",
        "[[doc] 🛠️](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=transpose#convtranspose2d) `torch.nn.ConvTranspose2d`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVi1zRYru9YG"
      },
      "source": [
        "```\n",
        "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n",
        "                         stride=1, padding=0, ...)\n",
        "```\n",
        "где:\n",
        "* `in_channels`, `out_channels` — количество каналов во входной и выходной картах признаков,\n",
        "* `kernel_size` — размер ядра свертки Transposed convolution,\n",
        "* `stride` — шаг свертки Transposed convolution,\n",
        "* `padding`— размер отступов, устанавливаемых по краям входной карты признаков.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTFYnIvku9YH"
      },
      "source": [
        "Пример использования:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRqBJwieu9YH"
      },
      "outputs": [],
      "source": [
        "input = torch.randn(1, 16, 16, 16)  # define dummy input\n",
        "print(\"Original size\", input.shape)\n",
        "\n",
        "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)  # define downsample layer\n",
        "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)  # define upsample layer\n",
        "\n",
        "# let`s downsample and upsample input\n",
        "with torch.no_grad():\n",
        "    output_1 = downsample(input)\n",
        "    print(\"Downsampled size\", output_1.size())\n",
        "\n",
        "    output_2 = upsample(output_1, output_size=input.size())\n",
        "    print(\"Upsampled size\", output_2.size())\n",
        "\n",
        "# plot results\n",
        "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
        "sns.heatmap(input[0, 0, :, :], ax=ax[0], cbar=False, vmin=-2, vmax=2)\n",
        "ax[0].set_title(\"Input\")\n",
        "sns.heatmap(output_1[0, 0, :, :], ax=ax[1], cbar=False, vmin=-2, vmax=2)\n",
        "ax[1].set_title(\"Downsampled\")\n",
        "sns.heatmap(output_2[0, 0, :, :], ax=ax[2], cbar=False, vmin=-2, vmax=2)\n",
        "ax[2].set_title(\"Upsampled\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qWRQ8Tmu9YH"
      },
      "source": [
        "## Пирамида признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff9i6YyJu9YI"
      },
      "source": [
        "Возникает вопрос: не потеряется ли информация о мелких деталях изображения при передаче через центральный блок сети, где пространственное разрешение минимально? Такая проблема существует.\n",
        "\n",
        "Те, кто изучал классические методы машинного зрения, помнят, что  при извлечении дескрипторов особых точек ([SIFT 📚[wiki]](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform)) использовалась так называемая пирамида изображений."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pft7LP_u9YI"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/pyramid_of_features.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd1M2c6tu9YI"
      },
      "source": [
        "Идея состоит в последовательном уменьшении (масштабировании) изображения и последовательном извлечении признаков в разных разрешениях.\n",
        "\n",
        "При уменьшении пространственных размеров мы естественным образом получаем карты признаков с разным пространственным разрешением."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn-FBIFqu9YI"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_information.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdPVHc_Au9YJ"
      },
      "source": [
        "Их можно использовать одновременно как в качестве входа для новых сверток, так и для получения предсказаний.\n",
        "\n",
        "На этой модели построены FPN-сети.\n",
        "\n",
        "[[arxiv] 🎓 Feature Pyramid Networks for Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1612.03144)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_M0Cprcu9YJ"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/fcn_1.png\" width=\"1000\">\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf\"> Fully Convolutional Networks for Semantic Segmentation</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHRNEbGyu9YK"
      },
      "source": [
        "После того, как все карты признаков будут увеличены до одного размера, они суммируются."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qVd1JB5u9YK"
      },
      "source": [
        "Примеры использования:\n",
        "* [[doc] 🛠️ Fully-Convolutional Network model with ResNet-50 and ResNet-101 backbones](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/)\n",
        "* [[doc] 🛠️ Models and Pre-trained Weights](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
        "\n",
        "\n",
        "Модель была предобучена на части датасета COCO train2017 (на 20 категориях, представленных так же в датасете  Pascal VOC). Использовались следующие классы:\n",
        "\n",
        "`['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20S0i5GEu9YK"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def coco2pil(url):\n",
        "    print(url)\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "\n",
        "# load resnet50\n",
        "fcn_model = torchvision.models.segmentation.fcn_resnet50(\n",
        "    weights=\"FCN_ResNet50_Weights.DEFAULT\", num_classes=21\n",
        ")\n",
        "\n",
        "classes = [\n",
        "    \"__background__\",\n",
        "    \"aeroplane\",\n",
        "    \"bicycle\",\n",
        "    \"bird\",\n",
        "    \"boat\",\n",
        "    \"bottle\",\n",
        "    \"bus\",\n",
        "    \"car\",\n",
        "    \"cat\",\n",
        "    \"chair\",\n",
        "    \"cow\",\n",
        "    \"diningtable\",\n",
        "    \"dog\",\n",
        "    \"horse\",\n",
        "    \"motorbike\",\n",
        "    \"person\",\n",
        "    \"pottedplant\",\n",
        "    \"sheep\",\n",
        "    \"sofa\",\n",
        "    \"train\",\n",
        "    \"tvmonitor\",\n",
        "]\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        ),  # ImageNet\n",
        "    ]\n",
        ")\n",
        "\n",
        "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
        "input_tensor = transform(pil_img)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = fcn_model(input_tensor.unsqueeze(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wRgzNVpu9YL"
      },
      "source": [
        "Возвращается словарь `output`, в котором по ключу `out` содержится массив со значениями ненормированных вероятностей, соответствующих предсказаниям каждого класса.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVfaPyGZu9YL"
      },
      "outputs": [],
      "source": [
        "print(\"output keys: \", output.keys())  # Ordered dictionary\n",
        "print(\"out: \", output[\"out\"].shape, \"Batch, class_num, h, w\")\n",
        "\n",
        "output_predictions = output[\"out\"][0].argmax(0)  # for first element of batch\n",
        "print(f\"output_predictions: {output_predictions.shape}\")\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.imshow(pil_img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "indexes = output_predictions\n",
        "semantic_seg_person_predict = np.zeros(pil_img.size).astype(bool)\n",
        "\n",
        "# plot all classes predictions\n",
        "fig, ax = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\n",
        "i = 0  # counter\n",
        "for row in range(4):\n",
        "    for col in range(5):\n",
        "        mask = torch.zeros(indexes.shape)\n",
        "        mask[indexes == i] = 255\n",
        "\n",
        "        ax[row, col].set_title(classes[i])\n",
        "        ax[row, col].imshow(mask)\n",
        "        ax[row, col].axis(\"off\")\n",
        "        i += 1\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9-h2BbXu9YL"
      },
      "outputs": [],
      "source": [
        "semantic_seg_person_predict = torch.zeros(indexes.shape)\n",
        "semantic_seg_person_predict[indexes == 15] = 1  # to obtain binary mask\n",
        "semantic_seg_person_predict = (\n",
        "    semantic_seg_person_predict.numpy()\n",
        ")  # for sklearn compability\n",
        "\n",
        "plt.imshow(semantic_seg_person_predict, cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_xAarJ8u9YL"
      },
      "source": [
        "##Метрики"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUoIfGwzu9YM"
      },
      "source": [
        "### IoU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-iPz-JRu9YM"
      },
      "source": [
        "Как оценить качество предсказаний, полученных от модели?\n",
        "\n",
        "Базовой метрикой является Intersection over Union (IoU), она же коэффициент Жаккарда ([Jaccard index 📚[wiki]](https://en.wikipedia.org/wiki/Jaccard_index)).\n",
        "\n",
        "Имеются предсказание модели (фиолетовая маска) и целевая разметка,  сделанная человеком (красная маска)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNNPs0ouu9YM"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/iou_sample.png\" width=\"400\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://datahacker.rs/deep-learning-intersection-over-union/\">Intersection over Union</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol2x1QWBu9YN"
      },
      "source": [
        "Необходимо оценить качество предсказания.\n",
        "\n",
        "**Для простоты в примере маски прямоугольные, но та же логика будет работать  для масок произвольной формы.*\n",
        "\n",
        "Метрика считается как отношение площади пересечения к площади объединения двух масок:\n",
        "\n",
        "$$ \\large \\text{IoU} = \\frac{|T \\cap P|}{|T \\cup P|} $$\n",
        "\n",
        "$T$ — True mask, $P$ — predicted mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNHZRVP0u9YN"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/iou_formula.png\" width=\"500\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfyvzgXDu9YO"
      },
      "source": [
        "Если маски совпадут на $100\\%$, то значение метрики будет равно $1$, и это наилучший результат. При пустом пересечении $\\text{IoU} $ будет нулевым. Значения метрики лежат в интервале $[0..1]$.\n",
        "\n",
        "В терминах ошибок первого/второго рода $\\text{IoU}$  можно записать как:\n",
        "\n",
        "$$ \\large \\text{IoU} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP} + \\text{FN}} $$\n",
        "\n",
        "\n",
        "$\\text{TP}$ — True positive — пересечение (обозначено желтым),\n",
        "\n",
        "$\\text{FP}$ — False Positive (остаток фиолетового прямоугольника),\n",
        "\n",
        "$\\text{FN}$ — False Negative (остаток красного прямоугольника)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pxWHnQ5u9YP"
      },
      "source": [
        "На базе этой метрики строится ряд производных от нее метрик, таких как Mean Average Precision, которую мы рассмотрим в разделе Детектирование.\n",
        "\n",
        "[[blog] ✏️ Intersection over Union](http://datahacker.rs/deep-learning-intersection-over-union/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRDfR4u_u9YP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"GT mask\")\n",
        "plt.imshow(semantic_seg_person_mask)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Predicted mask\")\n",
        "plt.imshow(semantic_seg_person_predict)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"GT & Predict overlap\")\n",
        "plt.axis(\"off\")\n",
        "tmp = semantic_seg_person_predict * 2 + semantic_seg_person_mask\n",
        "plt.imshow(tmp)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCdVvGz5u9YQ"
      },
      "source": [
        "Реализации:\n",
        "\n",
        "* [[git] 🐾 Jaccard score в Torchmetrics](https://github.com/Lightning-AI/torchmetrics/blob/8fade87062a7b87c1e6429bbe1c4e0112b3713a5/torchmetrics/functional/classification/jaccard.py)\n",
        "* [[doc] 🛠️ Jaccard score в Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)\n",
        "* [[doc] 🛠️ SMP в PyTorch](https://smp.readthedocs.io/en/latest/metrics.html#segmentation_models_pytorch.metrics.functional.iou_score)\n",
        "* [[doc] 🛠️ Jaccard index between two sets of boxes в Torchvision](https://pytorch.org/vision/main/generated/torchvision.ops.box_iou.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGVozPxuu9YQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "# wait vectors, so we flatten the data\n",
        "y_true = semantic_seg_person_mask.flatten()\n",
        "y_pred = semantic_seg_person_predict.flatten()\n",
        "iou = jaccard_score(y_true, y_pred)\n",
        "\n",
        "print(f\"IoU = {iou:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JbohNCcu9YR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
        "# or use:  smp.metrics.get_stats\n",
        "# https://smp.readthedocs.io/en/latest/metrics.html#segmentation_models_pytorch.metrics.functional.get_stats\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "iou = tp / (tp + fp + fn)\n",
        "print(f\"IoU = {iou:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyaBsJ9nu9YR"
      },
      "source": [
        "### Dice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeIJLrcCu9YR"
      },
      "source": [
        "Другой популярной метрикой для оценки качества сегментации является $\\text{Dice}$ коэффициент:\n",
        "\n",
        "$$ \\large \\text{Dice} = \\dfrac{2|A \\cap B|}{|A| + |B|} $$\n",
        "\n",
        "Концептуально он похож на $\\text{IoU}$, но при выражении через ошибки первого и второго рода будет видно, что он совпадет с [F1-мерой 🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html):\n",
        "\n",
        "$$ \\large \\text{Dice} =  \\frac{2|A \\cap B|}{|A| + |B|} = \\dfrac{2\\text{TP}}{2\\text{TP} + \\text{FP} + \\text{FN}} = \\text{F1_score} ∈ [0,1]$$\n",
        "\n",
        "[[blog] ✏️ F1 Score = Dice Coefficient](https://chenriang.me/f1-equal-dice-coefficient.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un9N_9RDu9YS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "dice_smp = f1_score(y_true, y_pred)\n",
        "dice = 2 * tp / (2 * tp + fp + fn)\n",
        "print(f\"Dice = {dice:.2f}\")\n",
        "print(f\"F1_score = {dice_smp:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kju9sBtPu9YS"
      },
      "source": [
        "Как видно, $\\text{IoU}$ сильнее наказывает за ошибки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMlivewZu9YT"
      },
      "source": [
        "[[blog] ✏️ All the segmentation metrics!](https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClCKHZNRu9YT"
      },
      "source": [
        "## Loss функции для сегментации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Aq6Jj4u9YT"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/loss_overview.png\" width=\"900\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://github.com/JunMa11/SegLoss\"> Loss functions for image segmentation </a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTo_NARNu9YU"
      },
      "source": [
        "### Distribution-based loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYPxlsY8u9YU"
      },
      "source": [
        "Так как задача сегментации сводится к задаче классификации, то можно использовать Cross-Entropy Loss, BCE или Focal Loss, с которыми мы знакомы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fTAPWgAu9YU"
      },
      "source": [
        "#### Binary Cross-Entropy (BCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipx_j0y_u9YV"
      },
      "source": [
        "Если предсказывается маска для объектов единственного класса (`target.shape` = 1×H×W), то задача сводится к бинарной классификации, так как каждый канал на выходе последнего слоя выдает предсказание для единственного класса.\n",
        "\n",
        "Это позволяет заменить Softmax в Cross-Entropy Loss на сигмоиду, а функцию потерь — на бинарную кросс-энтропию (BCE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_jx6yOJu9YV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "mask_class_1 = torch.randint(0, 2, (1, 64, 64))  # [0, 1]\n",
        "one_class_out = torch.randn(1, 1, 64, 64)\n",
        "print(mask_class_1.shape)\n",
        "print(one_class_out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf9LDPqRu9YV"
      },
      "source": [
        "Применяем [BCE with Logits Loss 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pkgEqlku9YW"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "bce_loss_wl = nn.BCEWithLogitsLoss()  # Sigmoid inside\n",
        "loss = bce_loss_wl(\n",
        "    one_class_out, mask_class_1.float().unsqueeze(0)\n",
        ")  # both params must have equal size\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Ek6sbPu9YW"
      },
      "source": [
        "Если последний слой модели — это Сигмоида, то можем использовать [BCE Loss 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF87Ol3du9YW"
      },
      "outputs": [],
      "source": [
        "norm_one_class_out = one_class_out.sigmoid()\n",
        "\n",
        "bce_loss = nn.BCELoss()\n",
        "loss = bce_loss(\n",
        "    norm_one_class_out, mask_class_1.float().unsqueeze(0)\n",
        ")  # both params must have equal size\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrQkWEvcu9YX"
      },
      "source": [
        "[Cross-Entropy Loss 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) для одного класса работать не будет:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ria9aDLu9YX"
      },
      "outputs": [],
      "source": [
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "loss = cross_entropy(one_class_out, mask_class_1.float().unsqueeze(0))\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hobD7TEIu9YX"
      },
      "source": [
        "Так как Softmax от единственного входа всегда равен $1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjvt4b2Au9YY"
      },
      "outputs": [],
      "source": [
        "one_class_out.softmax(dim=1).unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmKle5YFu9YY"
      },
      "source": [
        "Эту проблему можно решить искусственно, добавив в маску второй класс фона (background) отдельным каналом. Иными словами, для использования `CrossEntropyLoss` с `Softmax` в конце архиектуры нужно сделать One hot encoding одноканальной маски [OHE 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html). Тем самым, задача примет вид Multiclass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa4lBUcRu9YY"
      },
      "source": [
        "**Multilabel**\n",
        "\n",
        "Если предсказываются несколько классов и `target` имеет форму N×W×H (multilabel), где N — количество классов, то маска каждого хранится в отдельном канале:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ_pWIJdu9YZ"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_mask2.png\" width=\"600\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.jeremyjordan.me/semantic-segmentation/\"> An overview of semantic image segmentation </a></em></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEAbPWcEu9YZ"
      },
      "outputs": [],
      "source": [
        "mask_class1 = torch.randint(0, 2, (1, 64, 64))  # [0 , 1]\n",
        "mask_class2 = torch.randint(0, 2, (1, 64, 64))\n",
        "\n",
        "target = torch.cat((mask_class1, mask_class2))\n",
        "\n",
        "print(target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0DCdv2Tu9YZ"
      },
      "source": [
        "Видим, что форма выхода модели совпадает с формой тензора масок:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUiCq03vu9YZ"
      },
      "outputs": [],
      "source": [
        "two_class_out = torch.randn(1, 2, 64, 64)\n",
        "print(two_class_out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNV5Xngwu9YZ"
      },
      "source": [
        "Мы можем посчитать [BCE 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) поэлементно, предварительно преобразовав `target` во `float`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1le4v5pu9YZ"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
        "\n",
        "bce_loss = nn.BCEWithLogitsLoss()  # Sigmoid inside\n",
        "float_target = target.float()  # add batch and convert ot float\n",
        "loss = bce_loss(\n",
        "    two_class_out, float_target.unsqueeze(0)\n",
        ")  # both params must have equal size\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHv2X9Yru9Ya"
      },
      "source": [
        "Или [Cross-Entropy Loss 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6uIShDFu9Ya"
      },
      "outputs": [],
      "source": [
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "# If containing class probabilities, same shape as the input and each value should be between [0,1][0,1].\n",
        "loss = cross_entropy(two_class_out, float_target.unsqueeze(0))\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZctQrkm7u9Ya"
      },
      "source": [
        "Результаты не совпадают, так как после Sigmoid и Softmax получаются разные вероятности.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TucN_503u9Ya"
      },
      "source": [
        "#### Cross-Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ps22cpIu9Yb"
      },
      "source": [
        "Если маска задана одним каналом, в котором классы пронумерованы целыми числами (multiclass):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T21oFvdyu9Yb"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/semantic_mask1.png\" width=\"900\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.jeremyjordan.me/semantic-segmentation/\"> An overview of semantic image segmentation </a></em></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B48xwIUTu9Yb"
      },
      "outputs": [],
      "source": [
        "sq_target = target.argmax(0)\n",
        "sq_target.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MKtPls6u9Yb"
      },
      "source": [
        "То логично использовать `nn.CrossEntropyLoss`:\n",
        "\n",
        "```\n",
        "Input: Shape (C), (N,C) or (N,C,d1​,d2​,...,dK​) with K≥1 in the case of K-dimensional loss.\n",
        "\n",
        "Target: If containing class indices, shape (), (N) or (N,d1,d2,...,dK)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3eiwtK6u9Yb"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "loss = cross_entropy(two_class_out, sq_target.unsqueeze(0))\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QJ1i0_mu9Yc"
      },
      "source": [
        "**Различия Multilabel и Multiclass**\n",
        "\n",
        "Важно отметить, что помимо способа хранения/записи масок, режим multilabel подразумевает возможность принадлежности одного пикселя сразу нескольким классам, т. е. пересечение классов, что недопустимо в режиме multiclass.\n",
        "\n",
        "Таким образом, задача Multilabel является более сложной для нейросети. В библитеке Segmentation models PyTorch, рассматриваемой ниже, в функциях потерь для сегментации уже реализованы режимы **Binary, Multilabel и Multiclass** [SMP Losses 🛠️[doc]](https://segmentation-models-pytorch.readthedocs.io/en/latest/losses.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsZyK_oTu9Yc"
      },
      "source": [
        "### Region-based loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg84e99cu9Yc"
      },
      "source": [
        "К Region-based loss относятся функции потерь, основанные на оценке площади пересечения масок.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuIIH_9Eu9Yd"
      },
      "source": [
        "#### Jaccard Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JWWS42Au9Yd"
      },
      "source": [
        "В отличие от accuracy, рассчет IoU (Jaccard index):\n",
        "\n",
        "$\\large \\text{IoU} = \\text{JaccardIndex} = \\dfrac{  TP  }{TP + FP + FN} \\in [0,1]$\n",
        "\n",
        "можно произвести [дифференцируемым образом 🎓[arxiv]](https://arxiv.org/abs/1908.03851).\n",
        "\n",
        "Для этого нужно применить к предсказаниям функцию Sigmoid, а операцию пересечения заменить поэлементным умножением:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQcaWz4au9Ye"
      },
      "source": [
        "$$ \\large \\text{IoU} = \\frac{|A \\cap B|}{|A \\cup B|} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VaQYD8tu9Ye"
      },
      "source": [
        "$ |A \\cap B| =\n",
        "\\begin{bmatrix}\n",
        "0.01 & 0.03 & 0.02 & 0.02 \\\\ 0.05 & 0.12 & 0.09 & 0.07 \\\\ 0.89 & 0.85 & 0.88 & 0.91 \\\\ 0.99 & 0.97 & 0.95 & 0.97 \\end{bmatrix} * \\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}\n",
        "\\xrightarrow{\\ element-wise \\ multiply \\ } \\begin{bmatrix}\n",
        "0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0.89 & 0.85 & 0.88 & 0.91 \\\\ 0.99 & 0.97 & 0.95 & 0.97 \\end{bmatrix} \\xrightarrow{\\ sum \\ } 7.41$\n",
        "\n",
        "$\\qquad \\qquad \\qquad \\qquad \\text{prediction} \\qquad \\qquad \\qquad \\quad \\text{target}$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCB72DePu9Ye"
      },
      "source": [
        "$ |A| =\n",
        "\\begin{bmatrix}\n",
        "0.01 & 0.03 & 0.02 & 0.02 \\\\ 0.05 & 0.12 & 0.09 & 0.07 \\\\ 0.89 & 0.85 & 0.88 & 0.91 \\\\ 0.99 & 0.97 & 0.95 & 0.97 \\end{bmatrix}\n",
        "\\xrightarrow{\\ sum \\ }  7.82 \\qquad \\qquad|B| =\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\xrightarrow{\\ sum \\ }  8 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgLpNrAKu9Yf"
      },
      "source": [
        "$|A \\cup B| = |A +B - A  \\cap B|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxDzTA9Nu9Yf"
      },
      "source": [
        "И тогда метрику можно превратить в функцию потерь, инвертировав ее:\n",
        "\n",
        "$\\large \\text{Jaccard Loss} = 1 - \\text{IoU}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28sItZL9u9Yg"
      },
      "source": [
        "В PyTorch такой loss не реализован, поэтому для его использования установим библиотеку [SMP 🐾[git]](https://github.com/qubvel/segmentation_models.pytorch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRwp4QVTu9Yg"
      },
      "outputs": [],
      "source": [
        "!pip install -q segmentation-models-pytorch\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM1G5nlYu9Yg"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "iou_loss = smp.losses.JaccardLoss(smp.losses.MULTILABEL_MODE, from_logits=True)\n",
        "print(\"IoU Loss\", iou_loss(two_class_out, float_target.unsqueeze(0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db1eqbfsu9Yh"
      },
      "source": [
        "* При наличии на выходе модели сигмоиды или softmax, параметр `from_logits` следует устaновить равным `False`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKESJg6Xu9Yh"
      },
      "source": [
        "#### Dice loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7kHAiV6u9Yh"
      },
      "source": [
        "Аналогично можно посчитать Dice коэффициент:\n",
        "\n",
        "$$ \\large \\text{Dice} = \\dfrac{2|A \\cap B|}{|A| + |B|} $$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oia2slzXu9Yi"
      },
      "source": [
        "$$ \\large \\text{Dice} = \\frac{2 \\sum\\limits_\\text{pixels}y_\\text{true}y_\\text{pred}}{\\sum\\limits_\\text{pixels}y_\\text{true} + \\sum\\limits_\\text{pixels}y_\\text{pred}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzlmr4vRu9Yi"
      },
      "source": [
        "И затем превратить в функцию потерь:\n",
        "\n",
        "$$ \\large \\text{DiceLoss} = 1 - \\text{Dice} = 1 - \\dfrac{2\\sum\\limits_\\text{pixels}y_\\text{true}y_\\text{pred}}{\\sum\\limits_\\text{pixels}y_\\text{true} + \\sum\\limits_\\text{pixels}y_\\text{pred}} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwamPbI4u9Yi"
      },
      "source": [
        "Эта функция потерь также отсутствует в PyTorch, поэтому воспользуемся библиотекой [SMP 🐾[git]](https://github.com/qubvel/segmentation_models.pytorch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWkKFg6zu9Yj"
      },
      "outputs": [],
      "source": [
        "dice_loss = smp.losses.DiceLoss(smp.losses.MULTILABEL_MODE, from_logits=True)\n",
        "\n",
        "print(two_class_out.shape, target.shape)\n",
        "print(\"DICE Loss\", dice_loss(two_class_out, float_target.unsqueeze(0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdz2SvoSu9Yj"
      },
      "source": [
        "Концептуально Dice Loss и Jaccard Loss похожи. Но Jaccard Loss [сильнее штрафует модель на выбросах ✏️[blog]](https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIANTE39u9Yj"
      },
      "source": [
        "### Другие loss функции для сегментации и их различия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URzOWFozu9Yj"
      },
      "source": [
        "Основная часть функций потерь, относящихся к типам Region-based и Distribution-based, для задач семантической сегментации реализована в [SMP Losses 🛠️[doc]](https://segmentation-models-pytorch.readthedocs.io/en/latest/losses.html). Отметим некоторые эмпирические соображения:\n",
        "\n",
        "* Все Distribution-based losses представляют собой производные от cross entropy loss, что зачастую позволяет модели более стабильно обучаться.\n",
        "\n",
        "* Region-based losses являются дифференцируемыми аналогами основных метрик для оценки качества сегментации, поэтому использование их при обучении обеспечивает максимизацию нужных нам метрик, а именно Dice, IoU.\n",
        "\n",
        "* На практике, для обеспечения преимуществ каждого из типов функций потерь, их часто используют в линейной комбинации (Compound Loss):\n",
        "$$ \\text{SegLoss} = \\beta\\times \\text{RegionBasedLoss} + (1-\\beta) \\times \\text{DistributionBasedLoss},  \\quad \\beta \\in [0,1],$$\n",
        "при этом обычно весовой коэффициент $\\beta$ задают небольшим, отдавая приоритет DistributionBasedLoss.\n",
        "\n",
        "* Существуют также Boundary-based segmentation losses. Они используются при необходимости сохранить наиболее точные границы областей при сегментации, но на практике используются редко.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "045S-40su9Yk"
      },
      "source": [
        "Хороший обзор функций потерь для семантической сегментации можно найти в [Loss functions for image segmentation 🐾[git]](https://github.com/JunMa11/SegLoss)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFJpfSInu9Yk"
      },
      "source": [
        "## U-Net: Convolutional Networks for Biomedical Image Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biFuGOhJu9Yk"
      },
      "source": [
        "Можно спроектировать сеть так, что карты признаков на выходе сжимающих и соответствующих им разжимающих блоков будут иметь одинаковый размер."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCDfzw7Uu9Yl"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/efficient_way_predict_pixel_class.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU3_gDMJu9Yl"
      },
      "source": [
        "Признаки, полученные при сжатии, скопируем и объединим с признаками в разжимающих слоях, где карты признаков имеют соответствующее пространственное разрешение:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQMy33N8u9Yl"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/add_skip_connection.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cueKbykNu9Ym"
      },
      "source": [
        "Так же, как и в ResNet, этот механизм носит название skip connection, но  признаки  не суммируются, а конкатенируются."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNEFpCwqu9Ym"
      },
      "source": [
        "Рассмотренная нами схема используется в U-Net. Эта популярная модель для сегментации медицинских изображений изначально была предложена в [статье 🎓[arxiv]](https://arxiv.org/abs/1505.04597) для анализа  медицинских изображений."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaOO1Keau9Ym"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/unet_scheme.png\" width=\"700\"></center>\n",
        "\n",
        "<center><em>Архитектура U-Net</em></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://arxiv.org/abs/1505.04597\">U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WKKjswlu9Yn"
      },
      "source": [
        "[[git] 🐾 Реализация на PyTorch](https://github.com/milesial/Pytorch-UNet)\n",
        "\n",
        "[[doc] 🛠️ U-Net на PyTorch Hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yhpH2uju9Yn"
      },
      "source": [
        "Стоит обратить особое внимание на серые стрелки на схеме: они соответствуют операции конкатенации копий ранее полученных карт активаций по аналогии с DenseNet. Чтобы это было возможно, необходимо поддерживать соответствие между размерами карт активаций в процессах снижения и повышения пространственных размерностей. Для этой цели изменения размеров происходят только при операциях `MaxPool` и `MaxUnpool` — в обоих случаях в два раза."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNqJ5a1wu9Yn"
      },
      "source": [
        "В коде прямой проход может быть реализован, например, вот так:\n",
        "\n",
        "```\n",
        "def forward(self, x):\n",
        "    out1 = self.block1(x) #  ------------------------------>\n",
        "    out_pool1 = self.pool1(out1)\n",
        "\n",
        "    out2 = self.block2(out_pool1)\n",
        "    out_pool2 = self.pool2(out2)\n",
        "\n",
        "    out3 = self.block3(out_pool2)\n",
        "    out_pool3 = self.pool2(out3)\n",
        "\n",
        "    out4 = self.block4(out_pool3)\n",
        "    # return up\n",
        "    out_up1 = self.up1(out4)\n",
        "\n",
        "    out_cat1 = torch.cat((out_up1, out3), dim=1)\n",
        "    out5 = self.block5(out_cat1)\n",
        "    out_up2 = self.up2(out5)\n",
        "\n",
        "    out_cat2 = torch.cat((out_up2, out2), dim=1)\n",
        "    out6 = self.block6(out_cat2)\n",
        "    out_up3 = self.up3(out6)\n",
        "\n",
        "    out_cat3 = torch.cat((out_up3, out1), dim=1) # <-------\n",
        "    out = self.block7(out_cat3)\n",
        "\n",
        "    return out\n",
        "\n",
        "```\n",
        "После Upsample-блоков ReLU не используется."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjaBDGcnu9Yo"
      },
      "source": [
        "## Обзор DeepLabv3+ (2018)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaLUaft7u9Yo"
      },
      "source": [
        "DeepLab — семейство моделей для сегментации, значительно развивавшееся в течение четырёх лет. Основой данного рода моделей является использование **atrous (dilated) convolutions** и, начиная со второй модели, **atrous spatial pyramid pooling**, опирающейся на **spatial pyramid pooling**.\n",
        "\n",
        "[[arxiv] 🎓 Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Chen et al., 2018)](https://arxiv.org/abs/1802.02611v3)\n",
        "\n",
        "[[doc] 🛠️ Реализация на PyTorch](https://pytorch.org/vision/stable/models.html#deeplabv3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BHV8NjIu9Yo"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.1/L11/deeplabv3_scheme.png\" width=\"800\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://www.researchgate.net/publication/339754839_Semantic_Image_Segmentation_with_Deep_Convolutional_Neural_Networks_and_Quick_Shift\">Semantic Image Segmentation with DeepConvolutional Neural Networks and Quick Shift</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3JeSfQku9Yp"
      },
      "source": [
        "### Atrous (Dilated) Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlptofJLu9Yp"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/dev-2.1/L11/out/dilated_convolution.png\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLfgGUxDu9Yp"
      },
      "source": [
        "**Dilated convolution** (расширенная свертка) — это тип свертки, который \"раздувает\" ядро, как бы вставляя отверстия между элементами ядра, т. е. делает его разреженным. Дополнительный параметр `dilation` указывает, насколько сильно расширяется ядро.\n",
        "\n",
        "Фактически в такой свертке входные пиксели (признаки) участвуют через один (два, три ...). Параметр `dilation` указывает, какой по счету пиксель брать про свертке с ядром: при `dilation = 1` — каждый первый, при `dilation = 2` — каждый второй и т. д.\n",
        "\n",
        "Расширенные свертки позволяют значительно увеличить рецептивное поле и хорошо показывают себя при [решении задач семантической сегментации изображений 🎓[arxiv]](https://arxiv.org/pdf/1511.07122.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nfKBLsmu9Yp"
      },
      "source": [
        "\n",
        "```\n",
        "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1,\n",
        "                padding=0, dilation=1, ...)\n",
        "```\n",
        "где:\n",
        "* `in_channels`, `out_channels` — количество каналов во входной и выходной картах признаков,\n",
        "* `kernel_size` — размер ядра свертки,\n",
        "* `stride` — шаг свертки,\n",
        "* `padding` — размер отступов, устанавливаемых по краям входной карты признаков,\n",
        "* `dilation` — скорость расширения свертки.\n",
        "\n",
        "[[doc] 🛠️](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) `nn.Conv2d`"
      ]
    }
  ]
}