{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUe0QIP_0Wq5"
      },
      "source": [
        "# Loss для детектора"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcypi0YL0Wq6"
      },
      "source": [
        "Как подсчитать loss для детектора. Loss должн включать в себя две части: ошибку локализации и ошибку классификации.\n",
        "\n",
        "И для SSD loss function так и выглядит:\n",
        "\n",
        "$$\\large L(x,c,l,g) = \\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))$$\n",
        "\n",
        "\n",
        "Однако если мы будем считать ошибку локализации для всех default box, то она никогда не будет нулевой.\n",
        "Default box очень плотно перекрывает все изображение, и в большинство из них объект не попадет, особенно если объект один и небольшой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruaSWzG0Wq6"
      },
      "source": [
        "<center><img src =\"http://edunet.kea.su/repo/EduNet-additions/L11/default_boxes.png\" width=\"700\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://ternak.github.io/cnn_detection.html\">Object Detection With Convolution Neural Nets</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycNB3kC00Wq6"
      },
      "source": [
        "Поэтому часть default box при подсчете loss игнорируются. Используются только те, у которых площадь пересечения с одним из истинных bounding box больше порога ($\\text{IoU} > 0.5$)\n",
        "\n",
        "\n",
        "\n",
        "$$\\large L(x,l,g)_{loc} = \\sum_{i \\in Pos}^{N} x_{i,j}^{k}smooth_{L1}(l_i, g_j)$$\n",
        "\n",
        "\n",
        "Здесь:\n",
        "\n",
        "$l$ — финальные координаты предсказанного bounding box с учетом смещений,\n",
        "\n",
        "$g$ — координаты истинного bounding box,\n",
        "\n",
        "$M$ — количество истинных (ground true) bounding box-ов,\n",
        "\n",
        "$Pos$ — список отобранных default box, пересекающихся с истинными,\n",
        "\n",
        "$x_{i,j}^{k} = \\{1,0\\}$ — индикатор того, что комбинация default и box валидна.\n",
        "\n",
        "\n",
        "\n",
        "> $i$ — индекс default box,\n",
        "> $j$ — индекс истинного (ground true) bounding box,\n",
        "> $p$ — номер класса, к которому относится ground true bounding box (не степень).\n",
        "\n",
        "$smooth_{L1}$ — [Комбинация L1 и L2](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Компонент, отвечающий за классификацию:\n",
        "\n",
        "$$\\large L(x,с)_{conf} = -\\sum_{i \\in Pos} x_{i,j}^{k} log(softmax(c_{i}^{p})) -\\sum_{i \\in Neg} log(softmax(c_{i}^{0}))$$\n",
        "\n",
        "$c_{i}^{p}$ — вектор score для $i$-того default box, $p$ — номер истинного класса, соответствующего bounding box из разметки\n",
        "\n",
        "$Pos$ — список отобранных default box, не пересекающихся с истинными ($\\text{IoU} < threshold$)\n",
        "\n",
        "\n",
        "\\* *Формулы для loss function осознанно упрощены. Например, мы опустили расчет L1 для смещений, что является технической деталью.*"
      ],
      "metadata": {
        "id": "6mFB1mWRugSl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDEfq2H00Wq7"
      },
      "source": [
        "# FocalLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZBB4dJp0Wq7"
      },
      "source": [
        "Следующий заслуживающий внимания one-stage детектор — это RetinaNet ([Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002)).\n",
        "\n",
        "Собственно, авторы придумали новую функцию потерь (Focal Loss) и опубликовали модель, чтобы продемонстрировать её эффективность.\n",
        "\n",
        "Чтобы понять, какую проблему решает Focal Loss, давайте посмотрим на второй компонент Loss классификации для SSD:\n",
        "\n",
        "$$\\large L_{conf} =  \\ ...\\  -\\sum_{i \\in Neg} log(softmax(c_{i}^{0}))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это кросс-энтропия для bounding boxes, содержащих фон. Тут нет ошибки: когда модель обучится правильно предсказывать класс фона (background), каждая из этих компонент будет небольшой.\n",
        "\n",
        "Проблема в том, что таких компонент очень много. Детектор предсказывает несколько тысяч, или десятков тысяч bounding boxes. Подавляющая часть из них приходится на фон. Cумма большого количества этих небольших потерь (loss) становится заметным числом и мешает учиться классифицировать реальные объекты."
      ],
      "metadata": {
        "id": "vw9a5-PEu1QO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzSWLoXV0Wq8"
      },
      "source": [
        "Как решается эта проблема в Focal Loss?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GOnolKZ0Wq8"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/additions/L11/focal_loss_vs_ce.png\" width=\"700\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://arxiv.org/abs/1708.02002\">Focal Loss for Dense Object Detection (Lin et al., 2018)</a></em></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdsXi2230Wq8"
      },
      "source": [
        "Фактически loss для уверенно классифицированных объектов дополнительно занижается. Это похоже на взвешивание при дисбалансе классов.\n",
        "\n",
        "Достигается этот эффект путем домножения на коэффициент: $ (1-p_{t})^\\gamma$\n",
        "\n",
        "Здесь:\n",
        "\n",
        "$ p_{t} $ — вероятность истинного класса,\n",
        "\n",
        "$ \\gamma $ — число больше $1$, являющееся гиперпараметром."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пока модель ошибается, $p_{t}$ — мало, и значение выражения в скобках соответственно близко к $1$.\n",
        "\n",
        "Когда модель обучилась, значение $p_{t}$ становится близким к $1$, а разность в скобках становится маленьким числом, которое возводится в степень $ \\gamma > 1$. Таким образом, домножение на это небольшое число нивелирует вклад верно классифицированных объектов.\n",
        "\n",
        "Это позволяет модели сосредоточиться на изучении сложных объектов (hard examples)\n",
        "\n",
        "[Подробнее про FocalLoss](https://github.com/Gan4x4/ml_snippets/blob/main/FocalLoss.ipynb)"
      ],
      "metadata": {
        "id": "Ks9qlGWovGXM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dikEW3eH0WsI"
      },
      "source": [
        "# Нard Example Mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsuybhO-0WsJ"
      },
      "source": [
        "При обучении модели мы можем обнаружить, что средняя ошибка на всех данных достаточно маленькая, однако ошибка на редких нетипичных данных довольно высока. При этом нетипичные данные необязательно являются выбросами.\n",
        "\n",
        "Разберемся, почему так происходит.\n",
        "К примеру, рассмотрим задачу обнаружения автомобилей на потоках данных с камер наружного видеонаблюдения. Если в обучающем наборе большая часть данных — снимки, сделанные днём, то качество работы модели ночью будет низким. В данном случае, \"нетипичными\" данными будут ночные снимки. Но на самом деле \"нетипичных\" случаев может быть довольно много, и некоторые из них могут происходить даже днём. Например:\n",
        "* изменение погоды (изменение яркости, резкости, помехи на изображении),\n",
        "* смена сезона (снег либо листья могут покрыть дорогу — изменение фона),\n",
        "* машины с экзотическими узорами на кузове."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgc2L9-o0WsJ"
      },
      "source": [
        "Довольно простым и эффективным решением проблемы является сбор \"сложных\" случаев (**hard example mining**) и дообучение модели на них. При этом, поскольку модель уже довольно хорошо работает на большей части данных, можно дополнительно удалить часть данных из обучающей выборки — таким образом мы сосредотачиваем модель на обучении на сложных примерах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35KAdr2S0WsJ"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/additions/L11/hard_example_mining.png\" width=\"1000\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bqKpQ8B0WsK"
      },
      "source": [
        "## Online hard example mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-tHyDcP0WsK"
      },
      "source": [
        "В некоторых случаях hard exapmle mining можно выполнять прямо во время формирования батча, \"налету\". В таких случаях говорят про **online hard example mining**.\n",
        "\n",
        "Один из вариантов может быть реализован в two-stage детекторах.\n",
        "Напоминаю: первая часть детектора отвечает за обнаружение regions of interest (ROI), затем выполняется (как правило, сравнительно вычислительно дешёвая) классификация. Одним из вариантов реализации идеи может быть выполнение forward pass классификатора по всем предложенным ROI и затем формирование батча, в котором будет выделено определённое количество \"мест\" под ROI, предсказания на которых выполняются наихудшим образом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXMliruF0WsK"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/additions/L11/online_hard_example_mining.png\" width=\"700\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://sklin93.github.io/hog.html\"> HoG Face Detection with a Sliding Window </a></em></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3PtO_id0Wq9"
      },
      "source": [
        "# Feature pyramid network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JcEywtD0Wq-"
      },
      "source": [
        "Вторым полезным нововведением  в RetinaNet стало использование пирамиды признаков.\n",
        "\n",
        "[Feature Pyramid Networks for Object Detection (Tsung-Yi Lin et al., 2017)](https://arxiv.org/abs/1612.03144)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMaADzDb0Wq-"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/additions/L11/retinanet_use_outputs_fpn.png\" width=\"900\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://link.springer.com/article/10.1007/s11042-022-13153-y?error=cookies_not_supported&code=d283d48a-d725-4d7e-a568-7955a14a0550\">Tools, techniques, datasets and application areas for object detection in an image: a review</a></em></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTDE2RXu0Wq_"
      },
      "source": [
        "RetinaNet использует выходы FPN для предсказаний и класса, и bbox. Мы уже обсуждали пирамиды признаков применительно к сетям для сегментации, в частности, FCN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBIyIJcb0WrA"
      },
      "source": [
        "На каждом сверточном слое извлекаются карты признаков.\n",
        "\n",
        "Их пространственное разрешение постепенно уменьшается, а глубина (количество каналов) увеличивается.\n",
        "\n",
        "\n",
        "Но первые слои содержат мало семантической информации (только низкоуровневые признаки).  А карты признаков с глубоких слоев имеют низкое пространственное разрешение, что не позволяет качественно определить границы объектов.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE-9c3PS0WrB"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/additions/L11/semantic_information.png\" width=\"650\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://arxiv.org/pdf/1612.03144.pdf\">Feature Pyramid Networks for Object Detection</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ka8poi0WrB"
      },
      "source": [
        "Так же, как и в случае с сегментацией, точность повышается, если делать предсказания на картах, содержащих признаки для разных масштабов.\n",
        "\n",
        "При этом можно получать карты с большим пространственным разрешением не просто сохраняя их в памяти, но еще и прибавляя к ним значения признаков с более глубоких слоев, предварительно интерполировав их (Upsample)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQKEoxeO0WrC"
      },
      "source": [
        "Идея состоит в том, чтобы делать предсказание с учетом семантической информации, полученной на более глубоких слоях. Здесь признаки   суммируются, а не конкатенируются."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt39KGtp0WrC"
      },
      "source": [
        "Затем к новым картам признаков может применяться дополнительная свертка.\n",
        "\n",
        "На выходе получаем карты признаков P2–P5, на которых уже предсказываются bounding boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0AvV-sR0WrC"
      },
      "source": [
        "<img src =\"https://ml.gan4x4.ru/msu/additions/L11/resnet_prediction_head_scheme.png\" width=\"850\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHohiDES0WrC"
      },
      "source": [
        "В случае 2-stage детектора (RCNN) новые карты признаков подаются на вход RPN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54RaQBtl0WrD"
      },
      "source": [
        "<center><img src =\"http://edunet.kea.su/repo/EduNet-additions/L11/features_from_blackbone.jpeg\" width=\"1100\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c\">Understanding Feature Pyramid Networks for object detection (FPN)</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUDN-nLw0WrD"
      },
      "source": [
        "А признаки для предсказаний используются из backbone.\n",
        "\n",
        "Дополнительно: [Блог-пост про FPN](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)"
      ]
    }
  ]
}