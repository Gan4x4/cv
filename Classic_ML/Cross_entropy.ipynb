{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7Vo-7fEQPaA"
      },
      "source": [
        "# Расстояние (дивергенция) Кульбака — Лейблера:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sICnXXTQPaA"
      },
      "source": [
        "Чтобы понять, откуда взялась Cross-Entropy, рассмотрим сначала расстояние Кульбака — Лейблера.\n",
        "\n",
        "В математической статистике и теории информации мерой расхождения между двумя вероятностными распределениями $P$ и $Q$ является расстояние (дивергенция) Кульбака — Лейблера, вычисляемое по формуле:\n",
        "$$D_{KL}(P||Q) = ∑_i P(i)\\log\\frac{P(i)}{Q(i)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyFEH7C-QPaA"
      },
      "source": [
        "Попробуем разобраться, что значит эта формула, на примере двух монеток:\n",
        "- настоящей с вероятностями выпадения орла и решки 0.5 и 0.5 соответственно,\n",
        "- фальшивой с вероятностями выпадения орла и решки 0.2 и 0.8 соответственно.\n",
        "\n",
        "Возьмем настоящую монету и произведем 10 бросков (выборок). Получили последовательность $\\large \\color{blue}{О О} \\color{green}{Р} \\color{blue}{О О} \\color{green}{Р} \\color{blue}{О О О} \\color{green}{Р}$, где $\\large \\color{blue}{О}$ — это орел, $\\large \\color{green}{Р}$ — это решка.\n",
        "Посчитаем вероятности выбросить такую последовательность для настоящей и фальшивой монеты. Броски независимые, поэтому значения вероятностей перемножаются."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY1KMQ29QPaA"
      },
      "source": [
        "<img src=\"https://ml.gan4x4.ru/msu/additions/L02/kl_divergence.png\" width=\"900\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfKA5VJ2QPaA"
      },
      "source": [
        "Запишем пропорцию вероятностей данной комбинации для настоящей монетки и для фальшивой (независимые случайные величины, вероятности перемножаются). Для заданных значений вероятностей пропорция будет примерно $149:1$. При пропорции $1:1$ монетки будут идентичны.\n",
        "\n",
        "$$\\frac{\\color{blue}{p_1^{N_о}}\\color{green}{p_2^{N_р}}}\n",
        "{\\color{blue}{q_1^{N_о}}\\color{green}{q_2^{N_р}}}=\n",
        "\\frac{\\color{blue}{\\left(\\frac{1}{2}\\right)^{7}}\\color{green}{\\left(\\frac{1}{2}\\right)^{3}}}\n",
        "{\\color{blue}{\\left(\\frac{1}{5}\\right)^{7}}\\color{green}{\\left(\\frac{4}{5}\\right)^{3}}}\\approx \\frac{149}{1}$$\n",
        "\n",
        "Возьмем логарифм от этого значения (это позволит нам избавиться от степеней и заменить умножение сложением) и нормируем на количество бросков монетки $N=\\color{blue}{N_о}+\\color{green}{N_р}$.\n",
        "\n",
        "$$\\frac{\\color{blue}{N_о}}{N}\\log{\\color{blue}{p_1}}+\n",
        "\\frac{\\color{green}{N_р}}{N}\\log{\\color{green}{p_2}}-\n",
        "\\frac{\\color{blue}{N_о}}{N}\\log{\\color{blue}{q_1}}-\n",
        "\\frac{\\color{green}{N_р}}{N}\\log{\\color{green}{q_2}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia9gZsD5QPaA"
      },
      "source": [
        "При увеличении количества бросков $N\\to∞$, так как мы бросали настоящую монетку:\n",
        "\n",
        "$$\\frac{\\color{blue}{N_о}}{N} \\to \\color{blue}{p1}, \\frac{\\color{green}{N_р}}{N} \\to \\color{green}{p2}$$\n",
        "\n",
        "Получаем расстояние Кульбака — Лейблера:\n",
        "\n",
        "$$D_{KL}(P||Q) = \\color{blue}{p_1} \\log{\\color{blue}{p_1}}\n",
        "+ \\color{green}{p_2}\\log{\\color{green}{p_2}}\n",
        "- \\color{blue}{p_1}\\log{\\color{blue}{q_1}}\n",
        "- \\color{green}{p_2}\\log{\\color{green}{q_2}}$$\n",
        "\n",
        "$$ = \\color{blue}{p_1} \\log{\\color{blue}{\\frac{p_1}{q_1}}}\n",
        "+ \\color{green}{p_2} \\log{\\color{green}{\\frac{p_2}{q_2}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SxsrcjAQPaA"
      },
      "source": [
        "Посмотрим на разные фальшивые монетки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT56DwR5QPaA",
        "outputId": "02964332-e80c-474e-f121-cb5c6e8669f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q1 = 0.1, q2 = 0.9, Dkl = 0.511\n",
            "q1 = 0.2, q2 = 0.8, Dkl = 0.223\n",
            "q1 = 0.3, q2 = 0.7, Dkl = 0.087\n",
            "q1 = 0.4, q2 = 0.6, Dkl = 0.020\n",
            "q1 = 0.5, q2 = 0.5, Dkl = 0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def KL_dist(q1, q2, p1=0.5, p2=0.5):\n",
        "    return p1 * np.log(p1 / q1) + p2 * np.log(p2 / q2)\n",
        "\n",
        "\n",
        "print(f\"q1 = 0.1, q2 = 0.9, Dkl = {KL_dist(0.1, 0.9):0.3f}\")\n",
        "print(f\"q1 = 0.2, q2 = 0.8, Dkl = {KL_dist(0.2, 0.8):0.3f}\")\n",
        "print(f\"q1 = 0.3, q2 = 0.7, Dkl = {KL_dist(0.3, 0.7):0.3f}\")\n",
        "print(f\"q1 = 0.4, q2 = 0.6, Dkl = {KL_dist(0.4, 0.6):0.3f}\")\n",
        "print(f\"q1 = 0.5, q2 = 0.5, Dkl = {KL_dist(0.5, 0.5)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkXVDP6gQPaA"
      },
      "source": [
        "Обратим внимание, что если $P=Q$, то\n",
        "\n",
        "$$D_{KL}(P||Q) = ∑_i P(i)\\log\\frac{P(i)}{Q(i)} = 0$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHtWVd93QPaA"
      },
      "source": [
        "# Переход к оценке модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_gGGdqNQPaA"
      },
      "source": [
        "Мы научились определять близость двух распределений. Как это поможет нам оценить качество модели, если мы знаем, какие метки классов должны получиться?\n",
        "\n",
        "Пусть $P$ — вероятности истинных меток классов для объекта (1 для правильного класса, 0 для остальных), $Q(\\theta)$ — вероятности классов для объекта, предсказанные моделью с обучаемыми параметрами $\\theta$.\n",
        "\n",
        "Расстояние Кульбака — Лейблера между истинными и предсказанными значениями:\n",
        "\n",
        "$$D_{KL}(P||Q(\\theta)) = ∑_i P(i)\\log\\frac{P(i)}{Q(i| \\theta)} =$$\n",
        "\n",
        "$$ = ∑_i P(i)\\log{P(i))} -  ∑_i P(i)\\log{Q(i|\\theta)} =$$\n",
        "\n",
        "$$ = - H(P) + H(P|Q(\\theta))$$\n",
        "\n",
        "Мы разбили сумму на две части, первая из которых называется **энтропией** $H(P)$ и не будет зависеть от модели, а вторая называется **кросс-энтропией** $H(P|Q(\\theta))$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVt7yCAXQPaA"
      },
      "source": [
        "# Энтропия и кросс-энтропия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IXyQETyQPaA"
      },
      "source": [
        "Понятие энтропии пришло из теории связи. Для расчета энтропии можно использовать формулу Шеннона:\n",
        "\n",
        "$$H(P)=-\\sum^C_{i=1}P(i)\\cdot \\log_{2}(P(i)),$$\n",
        "где $C$ — количество классов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dfcxXz4QPaA"
      },
      "source": [
        "В формуле Шеннона не случайно используется логарифм по основанию 2. Она рассчитывает, **сколько бит информации минимально необходимо для передачи одного значения**, например, одного исхода броска монетки.\n",
        "\n",
        "Если монетка **всегда выдает орел**, то нам нет смысла передавать информацию, чтобы предсказать исход:\n",
        "$$H(\\color{blue}{p_1 = 1}, \\color{green}{p_2 = 0}) = 0$$\n",
        "\n",
        "Для настоящей монетки необходимо будет передавать 1 бит информации на бросок:\n",
        "$$H(\\color{blue}{p_1 = 0.5},  \\color{green}{p_2 = 0.5})= \\log_{2}(2) = 1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVeMn0sgQPaA"
      },
      "source": [
        "А вот с поддельной монеткой получается интереснее: если сделать много бросков, можно выявить закономерность, что решка выпадет чаще (вероятность трех решек подряд будет больше вероятности одного орла), и за счет этого сократить количество передаваемой информации так, чтобы на один бросок получалось меньше 1 бита. Как это сделать — отдельная область теории информации, называемая **кодирование источника**.\n",
        "$$H(\\color{blue}{q_1 = 0.2}, \\color{green}{q_2 = 0.8}) = 0.722$$\n",
        "\n",
        "__Энтропия__ — мера неуверенности, связанная с распределением $P$.\n",
        "Зная истинное распределение случайной величины, мы можем рассчитать его энтропию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO4vyMftQPaB"
      },
      "source": [
        "Если мы попытаемся использовать статистические данные, полученные для фальшивой монеты $Q$, для настоящей $P$, мы не получим выигрыш в количестве передаваемой информации:\n",
        "\n",
        "$$H(P||Q) = - \\sum^C_{i=1}P(i)\\cdot \\log_2(Q(i)) = 1.322$$\n",
        "\n",
        "Формула выше называется **кросс-энтропия**.\n",
        "\n",
        "**Кросс-энтропия** позволяет оценить ситуацию, когда мы аппроксимируем истинное распределение $P$ предсказанным распределением $Q$. Чем больше значения кросс-энтропии, тем больше расхождение между распределениями. Поэтому кросс-энтропию используют в качестве **функции потерь**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCq253zVQPaB"
      },
      "source": [
        "Как мы показали выше, **кросс-энтропия** связана с **энтропией** и **расстоянием Кульбака — Лейблера**:\n",
        "$$H(P||Q) = D_{KL}(P||Q) + H(P).$$\n",
        "\n",
        "Посчитаем значения для наших монеток с помощью кода:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPkb6PAiQPaB"
      },
      "outputs": [],
      "source": [
        "# normal coin\n",
        "p1 = 0.5\n",
        "p2 = 0.5\n",
        "\n",
        "# fake coin\n",
        "q1 = 0.2\n",
        "q2 = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tVVQ5IcQPaB",
        "outputId": "d9795700-b6c2-48c1-ee75-c806509aa0fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dkl(P||Q) = 0.322\n",
            "H(P) = 1.000\n",
            "H(Q) = 0.722\n",
            "H(P||Q) = 1.322\n",
            "H(P||Q) = Dkl(P||Q) + H(P) = 1.322\n"
          ]
        }
      ],
      "source": [
        "# Kullback–Leibler divergence\n",
        "div_kl = p1 * np.log2(p1 / q1) + p2 * np.log2(p2 / q2)\n",
        "print(f\"Dkl(P||Q) = {div_kl:.3f}\")\n",
        "\n",
        "# Entropy normal coin\n",
        "h_p = -p1 * np.log2(p1) - p2 * np.log2(p2)\n",
        "print(f\"H(P) = {h_p:.3f}\")\n",
        "\n",
        "# Entropy fake coin\n",
        "h_q = -q1 * np.log2(q1) - q2 * np.log2(q2)\n",
        "print(f\"H(Q) = {h_q:.3f}\")\n",
        "\n",
        "# Cross-entropy\n",
        "h_p_q = -p1 * np.log2(q1) - p2 * np.log2(q2)\n",
        "print(f\"H(P||Q) = {h_p_q:.3f}\")\n",
        "print(f\"H(P||Q) = Dkl(P||Q) + H(P) = {h_p+div_kl:.3f}\")"
      ]
    }
  ]
}