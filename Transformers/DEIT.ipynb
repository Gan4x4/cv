{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTR-q1MFELSW"
      },
      "source": [
        "## Обучение ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyP43GDKELSX"
      },
      "source": [
        "### Объем данных и ресурсов\n",
        "\n",
        "Как следует из текста [статьи](https://arxiv.org/abs/2010.11929), **ViT**, обученный на **ImageNet**, уступал baseline CNN-модели\n",
        "на базе сверточной сети (**ResNet**). И только при увеличении датасетов больше, чем **ImageNet**, преимущество стало заметным."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeSOWYXXELSX"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.0/L08/cited_vit_accuracy.png\"  width=\"400\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)</a></em></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm4-G43CELSX"
      },
      "source": [
        "Вряд ли в вашем распоряжении окажется датасет, сравнимый с [JFT-300M](https://paperswithcode.com/dataset/jft-300m) (300 миллионов изображений),\n",
        "и GPU/TPU ресурсы, необходимые для обучения с нуля (*it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days*)\n",
        "\n",
        "Поэтому для работы с пользовательскими данными используется техника дообучения ранее обученной модели на пользовательских данных (**fine-tuning**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbDS07FIELSY"
      },
      "source": [
        "## DeiT: Data-efficient Image Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYcjVVaaELSY"
      },
      "source": [
        "Для практических задач рекомендуем использовать эту реализацию. Авторы предлагают подход, благодаря которому становится возможным обучить модель на стандартном **ImageNet** (ImageNet1k) на одной рабочей станции за 3 дня.\n",
        "\n",
        "*We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhhumLagELSY"
      },
      "source": [
        "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.0/L08/cited_deit_vit.png\"  width=\"700\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://arxiv.org/abs/2012.12877\">Training data-efficient image transformers & distillation through attention</a></em></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI5LOzYpELSZ"
      },
      "source": [
        "Разбор этого материала уже не входит в наш курс и рекомендуется к самостоятельному изучению.\n",
        "\n",
        "Дополнительно:\n",
        "[Distilling Transformers: (DeiT) Data-efficient Image Transformers](https://towardsdatascience.com/distilling-transformers-deit-data-efficient-image-transformers-61f6cd276a03)\n",
        "\n",
        "Статьи, предшествовавшие появлению **ViT**:\n",
        "\n",
        "[Non-local Neural Networks](https://arxiv.org/abs/1711.07971)\n",
        "\n",
        "[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF3IDYXVELSZ"
      },
      "source": [
        "### Использование ViT с собственным датасетом"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ7qkds1ELSZ"
      },
      "source": [
        "Для использования **ViT** с собственными данными рекомендуем не обучать собственную модель с нуля, а использовать уже предобученную.\n",
        "\n",
        "Рассмотрим этот процесс на примере. Есть предобученный на **ImageNet** **Visual Transformer**, например: [deit_tiny_patch16_224](https://github.com/facebookresearch/deit)\n",
        "\n",
        "И мы хотим использовать ее со своим датасетом, который может сильно отличаться от **ImageNet**.\n",
        "\n",
        "Для примера возьмем **CIFAR-10**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8wMth_-ELSa"
      },
      "source": [
        "Загрузим модель. Как указано на [github](https://github.com/facebookresearch/deit), модель зависит от библиотеки [timm](https://fastai.github.io/timmdocs/), которую нужно установить."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrVuyRnSELSa"
      },
      "outputs": [],
      "source": [
        "!pip install -q timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-jVzHJXELSa"
      },
      "source": [
        "Теперь загружаем модель с [pytorch-hub](https://pytorch.org/hub/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Drz-qnoELSb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model = torch.hub.load(\n",
        "    \"facebookresearch/deit:main\", \"deit_tiny_patch16_224\", pretrained=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DULhezvJELSb"
      },
      "source": [
        "Убедимся, что модель запускается.\n",
        "Загрузим изображение:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9fV77AzELSb"
      },
      "outputs": [],
      "source": [
        "!wget  https://ml.gan4x4.ru/msu/dep-2.0/L08/capybara.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQc1XlxLELSc"
      },
      "source": [
        "И подадим его на вход трансформеру:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91tVX9bwELSd"
      },
      "outputs": [],
      "source": [
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "pil = Image.open(\"capybara.jpg\")\n",
        "\n",
        "# create the data transform that DeiT expects\n",
        "imagenet_transform = T.Compose(\n",
        "    [\n",
        "        T.Resize((224, 224)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
        "    ]\n",
        ")\n",
        "\n",
        "out = model(imagenet_transform(pil).unsqueeze(0))\n",
        "print(out.shape)\n",
        "pil.resize((224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Jg0re6ELSd"
      },
      "source": [
        "Чтобы использовать модель с **CIFAR-10**, нужно поменять количество выходов слоя, отвечающих за классификацию. Так как в **CIFAR-10** десять классов, а в **ImageNet** — тысяча.\n",
        "\n",
        "Чтобы понять, как получить доступ к последнему слою, выведем структуру модели:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldSm8svVELSd"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaUBULBHELSe"
      },
      "source": [
        "Видим, что последний слой называется head и, судя по количеству параметров на выходе (1000), которое совпадает с количеством классов **ImageNet**, именно он отвечает за классификацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDXeRdcXELSe"
      },
      "outputs": [],
      "source": [
        "print(model.head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY325QQzELSe"
      },
      "source": [
        "Заменим его слоем с 10-ю выходами по количеству классов в CIFAR-10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fgCmioEELSf"
      },
      "outputs": [],
      "source": [
        "model.head = torch.nn.Linear(192, 10, bias=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYteoFlAELSf"
      },
      "source": [
        "Убедимся, что модель не сломалась."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hMGA1_uELSf"
      },
      "outputs": [],
      "source": [
        "out = model(imagenet_transform(pil).unsqueeze(0))\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqno8MhNELSg"
      },
      "source": [
        "Теперь загрузим **CIFAR-10** и проверим, как дообучится модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LSmrgKlELSg"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "cifar10 = CIFAR10(root=\"./\", train=True, download=True, transform=imagenet_transform)\n",
        "\n",
        "# We use only part of CIFAR10 to reduce training time\n",
        "trainset, _ = torch.utils.data.random_split(cifar10, [10000, 40000])\n",
        "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = CIFAR10(root=\"./\", train=False, download=True, transform=imagenet_transform)\n",
        "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VaIAe0gELSh"
      },
      "source": [
        " Проведем стандартный цикл обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlKDc4B4ELSi"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, num_epochs=1):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in tqdm_notebook(train_loader):\n",
        "            inputs, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.to(device))\n",
        "            loss = criterion(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wLEW2EMELSi"
      },
      "source": [
        "Дообучаем (**fine tune**) только последний слой модели, который мы изменили."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBBi3f8pELSj"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.head.parameters(), lr=0.001, momentum=0.9)\n",
        "train(model, train_loader, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP0hHg0sELSj"
      },
      "source": [
        "Проверим точность, на всей тестовой подвыборке **CIFAR-10**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3_YnEpmELSj"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def accuracy(model, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in testloader:\n",
        "        images, labels = batch\n",
        "        outputs = model(images.to(device))\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.to(device)).sum().item()\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF1NtlSqELSk"
      },
      "outputs": [],
      "source": [
        "print(f\"Accuracy of fine-tuned network : {accuracy(model, test_loader):.2f} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vys8dp-dELSk"
      },
      "source": [
        "Дообучив последний слой на одной эпохе с использованием 20% данных, мы получили точность ~0.75\n",
        "\n",
        "Если дообучить все слои на 2-х эпохах, можно получить точность порядка 0.95.\n",
        "\n",
        "Это результат намного лучше чем тот, что мы получали на семинарах.\n",
        "\n",
        "Для этого потребуется порядка 10 мин (на GPU). Сейчас мы этого делать не будем.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_drN5KKELSl"
      },
      "source": [
        "И одной из причин того, что обучение идет относительно медленно, является увеличение изображений размером 32x32 до 224x224.\n",
        "\n",
        "Если бы мы использовали изображения **CIFAR-10** в их родном размере, мы бы не потеряли никакой информации, но могли бы в разы ускорить обучение.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8tyh16fELSm"
      },
      "source": [
        "### Изменение размеров входа ViT\n",
        "\n",
        "На первый взгляд, ничего не мешает это сделать: **self-attention** слой работает с произвольным количеством входов.\n",
        "\n",
        "Давайте посмотрим, что будет, если подать на вход модели изображение, отличное по размерам от 224x224.\n",
        "\n",
        "Для этого перезагрузим модель:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02p2nNqtELSm"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    model = torch.hub.load(\n",
        "        \"facebookresearch/deit:main\", \"deit_tiny_patch16_224\", pretrained=True\n",
        "    )\n",
        "    model.head = torch.nn.Linear(192, 10, bias=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = get_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CeQ9Hk5ELSn"
      },
      "source": [
        "И уберем из трансформаций Resize:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoOdlqeXELSn"
      },
      "outputs": [],
      "source": [
        "cifar_transform = T.Compose(\n",
        "    [\n",
        "        # T.Resize((224, 224)),    don't remove this line\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Change transformation in base dataset\n",
        "cifar10.transform = cifar_transform\n",
        "first_img = trainset[0][0]\n",
        "\n",
        "model.to(torch.device(\"cpu\"))\n",
        "try:\n",
        "    out = model(first_img.unsqueeze(0))\n",
        "except Exception as e:\n",
        "    print(\"Exception:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euPKON7mELSo"
      },
      "source": [
        "Получаем ошибку.\n",
        "\n",
        "Ошибка возникает в объекте [PatchEmbed](https://huggingface.co/spaces/Andy1621/uniformer_image_demo/blob/main/uniformer.py#L169), который превращает изображение в набор эмбеддингов.\n",
        "\n",
        "У объекта есть свойство `img_size`, попробуем просто поменять его:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMbY-8QXELSo"
      },
      "outputs": [],
      "source": [
        "model.patch_embed.img_size = (32, 32)\n",
        "try:\n",
        "    out = model(first_img.unsqueeze(0))\n",
        "except Exception as e:\n",
        "    print(\"Exception:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcL5iiRfELS-"
      },
      "source": [
        "Получаем новую ошибку.\n",
        "\n",
        "И возникает она в строке\n",
        "`x = self.pos_drop(x + self.pos_embed)`\n",
        "\n",
        "x — это наши новые эмбеддинги для CIFAR-10 картинок\n",
        "\n",
        "Откуда взялось число 5?\n",
        "\n",
        "4 — это закодированные фрагменты (patch) для картинки 32х32, их всего 4 (16x16) + один embedding для предсказываемого класса(class embedding).\n",
        "\n",
        "А 197 — это positional encoding — эмбеддинги, кодирующие позицию элемента. Они остались от **ImageNet**.\n",
        "\n",
        "Так как в ImageNet картинки размера 224x224, то в каждой помещалось 14x14 = 196 фрагментов и еще embedding для класса, итого 197 позиций.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1sDQnHCELS_"
      },
      "source": [
        "Эмбеддинги для позиций доступны через свойство:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TUuRTJCELS_"
      },
      "outputs": [],
      "source": [
        "model.pos_embed.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-JLvjasELTA"
      },
      "source": [
        "Теперь нам надо изменить количество pos embeddings так, чтобы оно было равно 5  (количество patch + 1).\n",
        "Возьмем 5 первых:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlbgwC68ELTA"
      },
      "outputs": [],
      "source": [
        "model.pos_embed.data = model.pos_embed.data[:, :5, :]\n",
        "out = model(first_img.unsqueeze(0))\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJclA-jDELTB"
      },
      "source": [
        "Заработало!\n",
        "\n",
        "Теперь обучим модель. Так как изображения стали намного меньше, то мы можем увеличить размер batch и использовать весь датасет. Также будем обучать все слои, а не только последний."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCW2LvRPELTC"
      },
      "outputs": [],
      "source": [
        "cifar10.transform = cifar_transform\n",
        "train_loader = DataLoader(cifar10, batch_size=512, shuffle=True, num_workers=2)\n",
        "\n",
        "# Now we train all parameters because model altered\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "train(model, train_loader, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mWwq_W8ELTC"
      },
      "source": [
        "Сильно быстрее.\n",
        "Посмотрим на результат:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tguKpKxELTD"
      },
      "outputs": [],
      "source": [
        "testset.transform = cifar_transform\n",
        "print(f\"Accuracy of altered network : {accuracy(model,test_loader):.2f} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNjS5U_9ELTD"
      },
      "source": [
        "Сильно хуже.\n",
        "\n",
        "Это можно объяснить тем, что  маленькие patch  ImageNet(1/196) семантически сильно отличаются от четвертинок картинок из CIFAR-10 (1/4).\n",
        "\n",
        "Но есть и другая причина: мы взяли лишь первые 4 pos_embedding а остальные отбросили. В итоге модель вынуждена практически заново обучаться работать с малым pos_embedding, и двух эпох для этого мало.\n",
        "\n",
        "Зато теперь мы можем использовать модель с изображениями любого размера."
      ]
    }
  ]
}